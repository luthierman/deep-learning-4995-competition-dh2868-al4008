{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4g8yMLqtS3W"
   },
   "source": [
    "# **Large-Scale Kinship Recognition Data Challenge: Kinship Verification STARTER NOTEBOOK**\n",
    "\n",
    "We provide framework code to get you started on the competition. The notebook is broken up into three main sections. \n",
    "1. Data Loading & Visualizing\n",
    "2. Data Generator & Model Building\n",
    "3. Training & Testing Model\n",
    "\n",
    "We have done the majority of the heavy lifting by making the data easily and readily accessible through Google Drive. Furthermore, we have made the task easier by creating a dataloader and fully trained end-to-end model that predicts a binary label (0 or 1) denoting whether two faces share a kinship relation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "David Heagy (dh2868), \n",
    "Alexander Lieberman (al4008)\n",
    "\n",
    "NOTE : Will need to change paths for running"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LDDgTAe2w0H"
   },
   "source": [
    "**WARNING: IF YOU HAVE NOT DONE SO**\n",
    "\n",
    "Change to GPU:\n",
    "\n",
    "Runtime --> Change Runtime Type --> GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWf8L2-Ru6ZE"
   },
   "source": [
    "Mount to Google Drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ribPmcZau-vR"
   },
   "source": [
    "Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8220,
     "status": "ok",
     "timestamp": 1628468141771,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "tS3ZhSjIAGgt"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install keras_vggface\n",
    "!pip install keras_applications\n",
    "!pip install keras-facenet\n",
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Multiply, Dropout, Subtract, BatchNormalization\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras_vggface.utils import preprocess_input\n",
    "\n",
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_facenet import FaceNet, inception_resnet_v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXViO7APvFYW"
   },
   "source": [
    "train_relationships.csv contains pairs of image paths which are positive samples (related to each other).\n",
    "\n",
    "train-faces contains the images for training itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1628469140072,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "ItZNSTCVAESV"
   },
   "outputs": [],
   "source": [
    "# Modify paths as per your method of saving them\n",
    "train_file_path = r\"<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\train_ds.csv\"\n",
    "train_folders_path = \"<REPLACE-PATH>\\\\Kinship Recognition Starter dh2868_al4008\\\\train\\\\train-faces\\\\\"\n",
    "# All images belonging to families F09** will be used to create the validation set while training the model\n",
    "# For final submission, you can add these to the training data as well\n",
    "val_famillies = \"F05\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "executionInfo": {
     "elapsed": 94682,
     "status": "ok",
     "timestamp": 1628469235675,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "dLuyuKKBAWMf"
   },
   "outputs": [],
   "source": [
    "all_images = glob(train_folders_path + r\"*\\*\\*.jpg\")\n",
    "\n",
    "train_images = [x for x in all_images if val_famillies not in x]\n",
    "val_images = [x for x in all_images if val_famillies in x]\n",
    "\n",
    "train_person_to_images_map = defaultdict(list)\n",
    "\n",
    "ppl = [x.split(\"\\\\\")[-3] + \"\\\\\" + x.split(\"\\\\\")[-2] for x in all_images]\n",
    "\n",
    "for x in train_images:\n",
    "    train_person_to_images_map[x.split(\"\\\\\")[-3] + \"\\\\\" + x.split(\"\\\\\")[-2]].append(x)\n",
    "\n",
    "val_person_to_images_map = defaultdict(list)\n",
    "\n",
    "for x in val_images:\n",
    "    val_person_to_images_map[x.split(\"\\\\\")[-3] + \"\\\\\" + x.split(\"\\\\\")[-2]].append(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "relationships = pd.read_csv(train_file_path)\n",
    "relationships = list(zip(relationships.p1.values, relationships.p2.values, relationships.relationship.values))\n",
    "relationships = [(\"\\\\\".join(x[0].split(\"/\")),\"\\\\\".join(x[1].split(\"/\")),x[2]) for x in relationships if \"\\\\\".join(x[0][:10].split(\"/\")) in ppl and \"\\\\\".join(x[1][:10].split(\"/\")) in ppl]\n",
    "\n",
    "train = [x for x in relationships if val_famillies not in x[0]]\n",
    "val = [x for x in relationships if val_famillies in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1628469239354,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "3KfHltcuAZcB"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "def read_img(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "# preprocess code for facenet taken from this link : \n",
    "# https://medium.com/@sritejabanisetti21/kinship-recognition-through-a-double-siamese-network-b2edacf39aee\n",
    "import random\n",
    "def prewhiten(x):\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y\n",
    "\n",
    "\n",
    "def read_img_vgg(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img, version=2)\n",
    "\n",
    "def read_img_fn(path):\n",
    "    img = image.load_img(path, target_size=(160, 160))\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return prewhiten(img)\n",
    "\n",
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
    "\n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], np.array(labels)\n",
    "        \n",
    "def gen2(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1_fn = np.array([read_img_fn(train_folders_path + x) for x in X1])\n",
    "        X1_vgg = np.array([read_img_vgg(train_folders_path + x) for x in X1])\n",
    "\n",
    "\n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2_fn = np.array([read_img_fn(train_folders_path + x) for x in X2])\n",
    "        X2_vgg = np.array([read_img_vgg(train_folders_path + x) for x in X2])\n",
    "        yield [X1_vgg, X2_vgg, X1_fn,X2_fn], np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "### new work\n",
    "from tensorflow import keras\n",
    "def model1():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "    model_seq = tf.keras.models.Sequential([input_1, input_2])\n",
    "\n",
    "    vgg_model = tf.keras.applications.vgg16.VGG16()\n",
    "    for layer in vgg_model.layers[0:-1]:\n",
    "        model_seq.add(layer)\n",
    "    for layer in model_seq.layers:\n",
    "        layer.trainable = False\n",
    "    model_seq.add(Dense(1, activation=\"sigmoid\"))\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    optim = keras.optimizers.Adam(lr = .001)\n",
    "    metrics=[\"accuracy\"]\n",
    "    model_seq.compile(optimizer=optim, loss = loss, metrics=metrics)\n",
    "    return model_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten, Lambda, Reshape,Add, Lambda, Subtract, Multiply, Average,Reshape\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow import keras\n",
    "\n",
    "# THIS MODEL HAD THE HIGHEST VAL ACCURACY : .596\n",
    "\n",
    "def model2():\n",
    "    #vgg\n",
    "    input_1_vgg = Input(shape=(224, 224, 3))\n",
    "    input_2_vgg = Input(shape=(224, 224, 3))\n",
    "    #facenet\n",
    "    input_1_fn = Input(shape=(160,160,3))\n",
    "    input_2_fn = Input(shape=(160,160,3))\n",
    "    vgg_model = VGGFace(model=\"senet50\", include_top=False)\n",
    "    facenet_path = r\"<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\facenet_keras.h5\"\n",
    "    facenet_model = load_model(facenet_path)\n",
    "    for x in vgg_model.layers[:-1]:\n",
    "        x.trainable = False\n",
    "    for x in facenet_model.layers:\n",
    "        x.trainable = False\n",
    "    x1 = vgg_model(input_1_vgg)\n",
    "    x2 = vgg_model(input_2_vgg)\n",
    "    x3 = facenet_model(input_1_fn)\n",
    "    x4 = facenet_model(input_2_fn)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    \n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "\n",
    "    \n",
    "    x3 = BatchNormalization()(x3)\n",
    "    \n",
    "    x4 = BatchNormalization()(x4)\n",
    "   \n",
    "\n",
    "    fn1 = Reshape((1,1,128))(x3)\n",
    "    fn2 = Reshape((1,1,128))(x4)\n",
    "\n",
    "    gmax_fn1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn1),GlobalAvgPool2D()(fn1) ])\n",
    "    gmax_fn2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn2),GlobalAvgPool2D()(fn2) ])\n",
    "    vgg_add = Add()([x1, x2])\n",
    "    vgg_sub = Subtract()([x1, x2])\n",
    "    vgg_mul = Multiply()([x1, x2])\n",
    "    vgg_avg = Average()([x1, x2])\n",
    "#     vgg_sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([x1,x2]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "#     vgg_sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([x1,x2]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "    fn_add = Add()([gmax_fn1, gmax_fn2])\n",
    "    fn_sub = Subtract()([gmax_fn1, gmax_fn2])\n",
    "    fn_mul = Multiply()([gmax_fn1, gmax_fn2])\n",
    "    fn_avg = Average()([gmax_fn1, gmax_fn2])\n",
    "#     fn_sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([gmax_fn1, gmax_fn2]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "#     fn_sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([gmax_fn1, gmax_fn2])\n",
    "\n",
    "#     conv_vgg_add = Conv2D(128, [1,1], )(vgg_add)\n",
    "#     conv_vgg_sub = Conv2D(128, [1,1], )(vgg_sub)\n",
    "#     conv_vgg_mul = Conv2D(128, [1,1], )(vgg_mul)\n",
    "#     conv_vgg_avg = Conv2D(128, [1,1], )(vgg_avg)\n",
    "#     conv_vgg_sub_sqd  = Conv2D(128, [1,1], )(vgg_sub_sqd )\n",
    "#     conv_avg_sqd_sub = Conv2D(128, [1,1], )(vgg_sqd_sub)\n",
    "\n",
    "    all_f = Concatenate(axis=-1)([Flatten()(vgg_add),  (fn_add) ,\n",
    "                                  Flatten()(vgg_sub), (fn_sub),\n",
    "                                  Flatten()(vgg_mul), (fn_mul),\n",
    "                                  Flatten()(vgg_avg), (fn_avg),\n",
    "#                                   Flatten()(conv_vgg_sub_sqd), (fn_sub_sqd),\n",
    "#                                   Flatten()(conv_avg_sqd_sub), (fn_sqd_sub)\n",
    "                                  \n",
    "                         ])\n",
    "    x = Dense(100, activation=\"relu\")(all_f)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation=\"relu\")(all_f)\n",
    "    x = Dropout(0.01)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1_vgg, input_2_vgg, input_1_fn, input_2_fn], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model3():\n",
    "    #vgg\n",
    "    input_1_vgg = Input(shape=(224, 224, 3))\n",
    "    input_2_vgg = Input(shape=(224, 224, 3))\n",
    "    #facenet\n",
    "    input_1_fn = Input(shape=(160,160,3))\n",
    "    input_2_fn = Input(shape=(160,160,3))\n",
    "\n",
    "    vgg_model_res = VGGFace(model=\"resnet50\", include_top=False)\n",
    "    vgg_model_sen = VGGFace(model=\"senet50\", include_top=False)\n",
    "    facenet_path = r\"<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\facenet_keras.h5\"\n",
    "\n",
    "    facenet_model = load_model(facenet_path)\n",
    "    for x in vgg_model_res.layers[:-1]:\n",
    "        x.trainable = False\n",
    "    for x in vgg_model_sen.layers[:-1]:\n",
    "        x.trainable = False\n",
    "    for x in facenet_model.layers:\n",
    "        x.trainable = False\n",
    "    x1r = vgg_model_res(input_1_vgg)\n",
    "    x2r = vgg_model_res(input_2_vgg)\n",
    "    x1s = vgg_model_sen(input_1_vgg)\n",
    "    x2s = vgg_model_sen(input_2_vgg)\n",
    "    \n",
    "    x3 = facenet_model(input_1_fn)\n",
    "    x4 = facenet_model(input_2_fn)\n",
    "   \n",
    "\n",
    "    fn1 = Reshape((1,1,128))(x3)\n",
    "    fn2 = Reshape((1,1,128))(x4)\n",
    "\n",
    "    gmax_fn1 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn1),GlobalAvgPool2D()(fn1) ])\n",
    "    gmax_fn2 = Concatenate(axis=-1)([GlobalMaxPool2D()(fn2),GlobalAvgPool2D()(fn2) ])\n",
    "    vggr_add = Add()([x1r, x2r])\n",
    "    vggs_add = Add()([x1s, x2s])\n",
    "    vggr_sub = Subtract()([x1r, x2r])\n",
    "    vggs_sub = Subtract()([x1s, x2s])\n",
    "    \n",
    "    vggr_mul = Multiply()([x1r, x2r])\n",
    "    vggs_mul = Multiply()([x1s, x2s])\n",
    "\n",
    "    vggr_avg = Average()([x1r, x2r])\n",
    "    vggs_avg = Average()([x1s, x2s])\n",
    "#     vggr_sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([x1r,x2r])\n",
    "#     vggs_sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([x1s,x2s]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "#     vggr_sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([x1r,x2r]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "#     vggs_sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([x1s,x2s])\n",
    "    fn_add = Add()([gmax_fn1, gmax_fn2])\n",
    "    fn_sub = Subtract()([gmax_fn1, gmax_fn2])\n",
    "    fn_mul = Multiply()([gmax_fn1, gmax_fn2])\n",
    "    fn_avg = Average()([gmax_fn1, gmax_fn2])\n",
    "#     fn_sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([gmax_fn1, gmax_fn2]) #https://stackoverflow.com/questions/47149675/keras-lambda-layer-for-l2-norm\n",
    "#     fn_sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([gmax_fn1, gmax_fn2])\n",
    "\n",
    "    conv_vggr_add = Conv2D(128, [1,1], )(vggr_add)\n",
    "    conv_vggr_sub = Conv2D(128, [1,1], )(vggr_sub)\n",
    "    conv_vggr_mul = Conv2D(128, [1,1], )(vggr_mul)\n",
    "    conv_vggr_avg = Conv2D(128, [1,1], )(vggr_avg)\n",
    "#     conv_vggr_sub_sqd  = Conv2D(128, [1,1], )(vggr_sub_sqd )\n",
    "#     conv_vggr_sqd_sub = Conv2D(128, [1,1], )(vggr_sqd_sub)\n",
    "\n",
    "\n",
    "    conv_vggs_add = Conv2D(128, [1,1], )(vggs_add)\n",
    "    conv_vggs_sub = Conv2D(128, [1,1], )(vggs_sub)\n",
    "    conv_vggs_mul = Conv2D(128, [1,1], )(vggs_mul)\n",
    "    conv_vggs_avg = Conv2D(128, [1,1], )(vggs_avg)\n",
    "#     conv_vggs_sub_sqd  = Conv2D(128, [1,1], )(vggs_sub_sqd )\n",
    "#     conv_vggs_sqd_sub = Conv2D(128, [1,1], )(vggs_sqd_sub)\n",
    "\n",
    "    all_f = Concatenate(axis=-1)([Flatten()(conv_vggr_add),  Flatten()(conv_vggs_add), (fn_add) ,\n",
    "                                  Flatten()(conv_vggr_sub),  Flatten()(conv_vggs_sub), (fn_sub),\n",
    "                                  Flatten()(conv_vggr_mul), Flatten()(conv_vggs_mul),(fn_mul),\n",
    "                                  Flatten()(conv_vggr_avg), Flatten()(conv_vggs_avg),(fn_avg),\n",
    "#                                   Flatten()(conv_vggr_sub_sqd), Flatten()(conv_vggs_sub_sqd),(fn_sub_sqd),\n",
    "#                                   Flatten()(conv_vggr_sqd_sub), Flatten()(conv_vggs_sqd_sub),(fn_sqd_sub),\n",
    "                                  \n",
    "                         ])\n",
    "    x = Dense(100, activation=\"relu\")(all_f)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1_vgg, input_2_vgg, input_1_fn, input_2_fn], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model4():\n",
    "    #vgg\n",
    "    input_1_vgg = Input(shape=(224, 224, 3))\n",
    "    input_2_vgg = Input(shape=(224, 224, 3))\n",
    "    #facenet\n",
    " \n",
    "\n",
    "    # vgg_model_res = VGGFace(model=\"resnet50\", include_top=False)\n",
    "    vgg_model_sen = VGGFace(model=\"senet50\", include_top=False)\n",
    "\n",
    "#     for x in vgg_model_res.layers:\n",
    "#         x.trainable = False\n",
    "    for x in vgg_model_sen.layers[:3]:\n",
    "        x.trainable = False\n",
    "  \n",
    "    x = vgg_model_sen(input_1_vgg)\n",
    "    y = vgg_model_sen(input_2_vgg)\n",
    "\n",
    "    \n",
    "    mul = Multiply()([x,y])\n",
    "    \n",
    "    sub_sqd = keras.layers.Lambda(lambda x: (x[0]-x[1])**2)([x,y])\n",
    "    sqd_sub = keras.layers.Lambda(lambda x: (x[0]**2-x[1]**2))([x,y])\n",
    "\n",
    "\n",
    "#     conv_vggr_mul = Conv2D(128, [1,1], )(vggr_mul)\n",
    "#     conv_vggr_avg = Conv2D(128, [1,1], )(vggr_avg)\n",
    "#     conv_vggr_sub_sqd  = Conv2D(128, [1,1], )(vggr_sub_sqd )\n",
    "#     conv_vggr_sqd_sub = Conv2D(128, [1,1], )(vggr_sqd_sub)\n",
    "\n",
    "\n",
    "\n",
    "#     conv_vggs_mul = Conv2D(128, [1,1], )(vggs_mul)\n",
    "#     conv_vggs_avg = Conv2D(128, [1,1], )(vggs_avg)\n",
    "#     conv_vggs_sub_sqd  = Conv2D(128, [1,1], )(vggs_sub_sqd )\n",
    "#     conv_vggs_sqd_sub = Conv2D(128, [1,1], )(vggs_sqd_sub)\n",
    "\n",
    "    all_f = Concatenate(axis=-1)([(sub_sqd), \n",
    "                                  (sqd_sub), \n",
    "                                  (mul),\n",
    "                         ])\n",
    "    x = Dense(128, activation=\"relu\")(all_f)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1_vgg, input_2_vgg], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.000001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_63\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_175 (InputLayer)          [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_176 (InputLayer)          [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "vggface_senet50 (Functional)    (None, None, None, 2 26092144    input_175[0][0]                  \n",
      "                                                                 input_176[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_68 (Lambda)              (None, 1, 1, 2048)   0           vggface_senet50[0][0]            \n",
      "                                                                 vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_69 (Lambda)              (None, 1, 1, 2048)   0           vggface_senet50[0][0]            \n",
      "                                                                 vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_632 (Multiply)         (None, 1, 1, 2048)   0           vggface_senet50[0][0]            \n",
      "                                                                 vggface_senet50[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_185 (Flatten)           (None, 2048)         0           lambda_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_186 (Flatten)           (None, 2048)         0           lambda_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_187 (Flatten)           (None, 2048)         0           multiply_632[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 6144)         0           flatten_185[0][0]                \n",
      "                                                                 flatten_186[0][0]                \n",
      "                                                                 flatten_187[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_102 (Dense)               (None, 128)          786560      concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_103 (Dense)               (None, 1)            129         dense_102[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 26,878,833\n",
      "Trainable params: 786,689\n",
      "Non-trainable params: 26,092,144\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/30\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018F9E33D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x0000018F9E33D678> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "100/100 [==============================] - ETA: 0s - loss: 13.6840 - acc: 0.6050 ETA: 2s - loss: 15.1297 - ETA: 2s - loss: 1 - ETA: 1s - loss: 14.3401 - acc: 0.59 - ETA: 1s - loss: 14.3043 - acc: 0.59 - ETA: 1s - losWARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018FB2473708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x0000018FB2473708> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: 'arguments' object has no attribute 'posonlyargs'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.50500, saving model to C:\\Users\\DavidHeagy\\Desktop\\Kinship Recognition Starter dh2868_al4008-20210809T164504Z-001\\Kinship Recognition Starter dh2868_al4008\\new_face_2021-08-15 17:55:24.962039.h5\n",
      "100/100 [==============================] - 15s 155ms/step - loss: 13.6840 - acc: 0.6050 - val_loss: 19.1742 - val_acc: 0.5050\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 6.4280 - acc: 0.7600- ETA: 2s - loss: 6.8508  - ETA: 1s - loss: 6.\n",
      "Epoch 00002: val_acc improved from 0.50500 to 0.55375, saving model to C:\\Users\\DavidHeagy\\Desktop\\Kinship Recognition Starter dh2868_al4008-20210809T164504Z-001\\Kinship Recognition Starter dh2868_al4008\\new_face_2021-08-15 17:55:24.962039.h5\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 6.4280 - acc: 0.7600 - val_loss: 17.2126 - val_acc: 0.5537\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 4.3117 - acc: 0.8325- ETA: 4s - loss: 4.2803 - acc: - ETA: 3s - loss: 4.3247 - acc: 0.83 - ETA:  - ETA: 0s - loss: 4.3132 - acc: 0\n",
      "Epoch 00003: val_acc improved from 0.55375 to 0.55500, saving model to C:\\Users\\DavidHeagy\\Desktop\\Kinship Recognition Starter dh2868_al4008-20210809T164504Z-001\\Kinship Recognition Starter dh2868_al4008\\new_face_2021-08-15 17:55:24.962039.h5\n",
      "100/100 [==============================] - 13s 135ms/step - loss: 4.3117 - acc: 0.8325 - val_loss: 16.4275 - val_acc: 0.5550\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 3.1637 - acc: 0.8788- ETA: 7s - loss: 4.3564 -  - ETA: 7s  - ETA: 4s - loss: 3.286 - ETA: 3s - loss: 3.2550 - acc: - ETA: 2s - loss: 3.2264  - ETA: 1s - loss: 3.2967 - ac - ETA: 0s - loss: 3.1572 - acc: 0. - ETA: 0s - loss: 3.1936 - acc: \n",
      "Epoch 00004: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 3.1637 - acc: 0.8788 - val_loss: 18.2613 - val_acc: 0.5512\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.9627 - acc: 0.9087- ETA: 2s - los\n",
      "Epoch 00005: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 1.9627 - acc: 0.9087 - val_loss: 21.9122 - val_acc: 0.5325\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.7778 - acc: 0.9150- ETA: 7s - loss: 1.8093 - - ETA: 7s - loss: 1.6 - ETA:  - ETA: 3s - loss - ETA: 0s - loss: 1.7100 - a\n",
      "Epoch 00006: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 140ms/step - loss: 1.7778 - acc: 0.9150 - val_loss: 21.9289 - val_acc: 0.5150\n",
      "Epoch 7/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - ETA: 0s - loss: 1.5459 - acc: 0.9306- ETA: 7s - loss: 2.4140 - acc - ETA: 6s - loss: - ETA: 4s - loss: 1 - ETA: 3s - loss: 1.7088  - ETA: 1s - loss:  - ETA: 0s - loss: 1.5616 - acc: 0.929\n",
      "Epoch 00007: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 1.5459 - acc: 0.9306 - val_loss: 24.1011 - val_acc: 0.4975\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.3274 - acc: 0.9356- ETA: 1s - loss: 1\n",
      "Epoch 00008: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 1.3274 - acc: 0.9356 - val_loss: 21.0223 - val_acc: 0.5400\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.2397 - acc: 0.9400- ETA: 4s - loss: 1.2434 - acc:  - ETA: 4s - loss - ETA: 2s - loss: 1.3230 - acc: - ETA: 1s - loss: 1.23 - ETA: 0s - loss: 1.2273 - acc: 0.9406\n",
      "Epoch 00009: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 1.2273 - acc: 0.9406 - val_loss: 24.9620 - val_acc: 0.5325\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7459 - acc: 0.9581- ETA: 6s - l - ETA: 0s - loss: 0.7158 - acc:\n",
      "Epoch 00010: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.7459 - acc: 0.9581 - val_loss: 23.9862 - val_acc: 0.5050\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.6462 - acc: 0.9675- ETA: 6s - loss: 0.7298 - - ETA: 5s - loss: 0.7345  - ETA: 4s - loss: 0 - ETA: \n",
      "Epoch 00011: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.6462 - acc: 0.9675 - val_loss: 26.3829 - val_acc: 0.5100\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.7450 - acc: 0.9606- ETA: 1s - loss: 0.71\n",
      "Epoch 00012: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.7450 - acc: 0.9606 - val_loss: 26.2499 - val_acc: 0.5400\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5374 - acc: 0.9712- ETA: 4s - loss: 0.4891 - - ETA: 3s - loss: 0.5122 - acc: 0.\n",
      "Epoch 00013: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.5374 - acc: 0.9712 - val_loss: 25.0603 - val_acc: 0.5412\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.3985 - acc: 0.9737- ETA: 7s - loss: 0.4728  - ETA: 6s - loss: 0.2974 - acc: 0.974 - ETA: 6s - loss: 0.2875  - ET - ETA: 2s - loss: 0.4216 - ETA: 0s - loss: 0.3985 - acc: 0.9 - ETA: 0s - loss: 0.3964 - acc: \n",
      "Epoch 00014: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 139ms/step - loss: 0.3985 - acc: 0.9737 - val_loss: 24.0866 - val_acc: 0.5387\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.5954 - acc: 0.9750- - ETA: 0s - loss: 0.5528 - acc: 0\n",
      "Epoch 00015: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.5954 - acc: 0.9750 - val_loss: 33.5659 - val_acc: 0.5150\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.4142 - acc: 0.9744- ETA: 0s - loss: 0.4242 - acc: 0.\n",
      "Epoch 00016: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.4142 - acc: 0.9744 - val_loss: 30.4162 - val_acc: 0.5250\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2762 - acc: 0.9825\n",
      "Epoch 00017: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.2762 - acc: 0.9825 - val_loss: 25.0108 - val_acc: 0.5512\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2803 - acc: 0.9862- ETA: 5s -\n",
      "Epoch 00018: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.2803 - acc: 0.9862 - val_loss: 30.3437 - val_acc: 0.5150\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2096 - acc: 0.9894- ETA: 3s - loss: 0.2528 - acc: 0.9 - ET\n",
      "Epoch 00019: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2096 - acc: 0.9894 - val_loss: 28.9302 - val_acc: 0.5350\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2505 - acc: 0.9862- ETA: 0s - loss: 0.2588 - acc: 0\n",
      "Epoch 00020: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2505 - acc: 0.9862 - val_loss: 29.7666 - val_acc: 0.5337\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2045 - acc: 0.9880- ETA: 7s - - ETA: 4s - l - ETA: 2s - loss: 0.222 - ETA: 1s - loss: 0.2065 - ETA: 0s - loss: 0.2024 - acc: 0.9881\n",
      "Epoch 00021: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.2024 - acc: 0.9881 - val_loss: 29.7848 - val_acc: 0.5213\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2557 - acc: 0.9869- ETA: 5s  - ETA: 3s - loss: 0.3552 - - ETA: 2s - los\n",
      "Epoch 00022: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.2557 - acc: 0.9869 - val_loss: 35.2583 - val_acc: 0.4888\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2322 - acc: 0.9862- ETA: 7s - loss: - ETA: 2s - loss: 0.3020 - acc: - ETA: 1s - loss: 0.2756 - acc: 0.9 - ETA: 1s - loss: 0.2\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2322 - acc: 0.9862 - val_loss: 32.9487 - val_acc: 0.5088\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2247 - acc: 0.9867- ETA: 0s - loss: 0.1872 - acc - ETA: 0s - loss: 0.2224 - acc: 0.9869\n",
      "Epoch 00024: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.2224 - acc: 0.9869 - val_loss: 35.8863 - val_acc: 0.4787\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2085 - acc: 0.9837- ETA: 1s - loss: 0.176\n",
      "Epoch 00025: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2085 - acc: 0.9837 - val_loss: 30.7337 - val_acc: 0.5263\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1068 - acc: 0.9887- ETA: - ETA: 3s - loss: - ETA: 1s - loss: 0.1242 - - ETA: 0s - loss: 0.1090 - acc: 0.988 - ETA: 0s - loss: 0.1079 - acc: 0.988\n",
      "Epoch 00026: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 137ms/step - loss: 0.1068 - acc: 0.9887 - val_loss: 31.8248 - val_acc: 0.5275\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1443 - acc: 0.9894- ETA: 1s - loss: 0.\n",
      "Epoch 00027: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 135ms/step - loss: 0.1443 - acc: 0.9894 - val_loss: 33.1318 - val_acc: 0.4913\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.2235 - acc: 0.9900- ETA: 6 - ETA: 4s - loss: 0.1726 - acc: 0.991 - ETA: 4s - loss: 0.1695 - acc: 0.9 - ETA: 4s - loss: 0.2054 - a - ETA: 3s - loss - ETA: 1s - loss: 0.2276 - acc:  - ETA: 0s - loss: 0.2186 - acc:\n",
      "Epoch 00028: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 136ms/step - loss: 0.2235 - acc: 0.9900 - val_loss: 34.2793 - val_acc: 0.5013\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1183 - acc: 0.9931- ETA: 7s - loss: 0.410 - ETA: 6 - ETA: 4s - loss: 0 - ETA: 2s - \n",
      "Epoch 00029: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1183 - acc: 0.9931 - val_loss: 34.7288 - val_acc: 0.4837\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - ETA: 0s - loss: 0.1787 - acc: 0.9887\n",
      "Epoch 00030: val_acc did not improve from 0.55500\n",
      "100/100 [==============================] - 14s 138ms/step - loss: 0.1787 - acc: 0.9887 - val_loss: 35.5372 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18f9e336d48>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "date_t = datetime.datetime.now()\n",
    "new_file_path = r\"<PATH>\\Kinship Recognition Starter dh2868_al4008\\new_face_{}.h5\".format(date_t)\n",
    "model = model4()\n",
    "epochs = 30\n",
    "checkpoint = ModelCheckpoint(new_file_path, \n",
    "                             monitor='val_acc', \n",
    "                             verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\",\n",
    "    patience=6,verbose = 2) \n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [ reduce_on_plateau, checkpoint]\n",
    "\n",
    "model.fit(gen(train, train_person_to_images_map, batch_size=16), use_multiprocessing=False,\n",
    "                validation_data=gen(val, val_person_to_images_map, batch_size=16), epochs=epochs, verbose=1,\n",
    "                workers=1, callbacks=callbacks_list, steps_per_epoch=100, validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEaCAYAAADzDTuZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAABZ30lEQVR4nO3dd3hUVfrA8e+dkpn0XkgDAgiEiEgx9BoRBBEVsQFS7Px0V10VXBV2RUCUBXVxRcXGouKKoqCohI6AoKFJD4QQSO+9zMz5/TFkSCBlUieZnM/z5MnMnbn3vudOct+555x7jiKEEEiSJEltmsrWAUiSJEm2J5OBJEmSJJOBJEmSJJOBJEmShEwGkiRJEjIZSJIkSchkINVi+/btKIrCxYsX67Seoij897//baKo2q7hw4fz0EMP2ToMyQ7JZGAnFEWp8adDhw712u7AgQNJSkoiMDCwTuslJSUxadKkeu2zrmTiqdrjjz+OWq1mxYoVtg5FagVkMrATSUlJlp9169YBEBMTY1l24MCBSu8vLS21arsODg4EBASgUtXtTyUgIAC9Xl+ndaTGU1BQwJo1a3jxxRf54IMPbB0OYP3fnGQbMhnYiYCAAMuPl5cXAL6+vpZlfn5+vP3229x///24u7szdepUAP7+97/TvXt3nJycCAkJ4bHHHiMnJ8ey3auricqfb968maFDh+Lk5ER4eDibNm2qFM/V39YVReHdd99l6tSpuLq6EhwczKJFiyqtk5GRwd13342zszP+/v68/PLLPPjgg0RFRTXo2Hz66aeEh4fj4OBAcHAwL730EgaDwfL67t27GTRoEK6urri6unLDDTfw888/W15fuHAhYWFh6HQ6fH19ueWWWygqKqp2f59//jmRkZG4u7vj4+PDuHHjOH36tOX18+fPoygKX331FePHj8fJyYmwsDA++eSTStuJj49nzJgxODo6EhISwjvvvGN1mb/44gu6dOnCSy+9RHx8PL/99ts171m7di19+vRBr9fj7e3N2LFjycrKsry+YsUKwsPD0el0+Pn5cdddd1le69ChAwsWLKi0vYceeojhw4dbng8fPpxZs2bx8ssv065dO0JDQ606PgCpqanMmDEDf39/9Ho9Xbt25aOPPkIIQVhYGAsXLqz0/oKCAtzc3Fi9erXVx0iqTCaDNuQf//gHAwcOJCYmxvKP7OjoyPvvv8/x48f55JNP2L59O0899VSt2/rb3/7Giy++yOHDh4mMjOSee+6pdCKpbv9Dhw7l0KFDzJ07lxdffJEtW7ZYXp8xYwaHDx9m48aNbN26lYsXL7J+/foGlfmHH35g5syZTJ06lT///JOlS5eyYsUK/vGPfwBgMBiYMGECkZGRxMTEEBMTw/z583FycgLgm2++YfHixbz11lucOXOGzZs3M3bs2Br3WVJSwksvvURMTAybN29GrVYzbty4a74Zz5kzh2nTpnHkyBHuvfdeHnroIctJUQjBHXfcQUZGBtu3b2fDhg18//33xMTEWFXulStXMn36dHQ6Hffeey8rV66s9PrHH3/MlClTmDhxIjExMWzbto0xY8ZgNBoBmDdvHi+88AJPPPEER48e5aeffqJ3795W7buir776irS0NLZs2cLmzZutOj5FRUUMGzaMw4cPs2bNGo4fP84777yDk5MTiqLw8MMPs2rVKiqOpPPll1+i0Wi4++676xyjdJmQ7M62bdsEIBISEizLADFz5sxa1/3mm2+Eg4ODMBqNVW6r/Pm6dess6yQnJwtA/PTTT5X2t3r16krPn3zyyUr76tatm5gzZ44QQojTp08LQERHR1teLy0tFcHBwWLUqFE1xnz1vioaPHiwuPvuuystW758udDr9aKkpERkZmYKQGzbtq3K9f/1r3+JLl26iNLS0hpjqElGRoYAxO7du4UQQsTFxQlALF261PIeg8EgXFxcxHvvvSeEEGLz5s0CEKdOnbK8JzU1Vej1ejFr1qwa93fw4EHh4OAg0tPThRBC7N27Vzg5OYns7GzLe0JCQsTs2bOrXD8/P1/o9XrxxhtvVLuP9u3bi1dffbXSslmzZolhw4ZZng8bNkx06dLF8rdUnauPz4cffih0Ol2lv9+KkpOThVarFZs3b7Ys69+/v3jqqadq3I9UM3ll0IbcdNNN1yz75ptvGDp0KIGBgbi4uPDAAw9QWlpKcnJyjdvq1auX5bG/vz9qtZqUlBSr1wEIDAy0rHP8+HEA+vfvb3ldq9XSt2/fGrdZm2PHjjF06NBKy4YNG0ZxcTFnz57F09OThx56iFtuuYWxY8eyePFiTp06ZXnv5MmTKSsro3379kyfPp3Vq1eTl5dX4z4PHTrEHXfcQceOHXF1dbVUj8THx1d6X8XjoVar8fPzq3Q8fHx8uO666yzv8fX1pWvXrrWWeeXKlYwfPx5vb2/AfEyDg4Mt1XapqakkJCQwevToKtc/duwYxcXF1b5eF3369Lmmvam24/PHH38QHh5OcHBwldv09/fn9ttvt7SF/Pnnn+zbt4+HH364wfG2ZTIZtCHOzs6Vnv/222/cfffdDB06lG+//ZaYmBjee+89oPbGPgcHh2uWmUymOq2jKMo16yiKUuM2msIHH3zAH3/8wc0338yOHTuIiIiwVKsEBQVx8uRJPvroI/z8/Hj11Vfp2rUrCQkJVW6rsLCQ0aNHoygKH3/8Mfv37+fAgQMoinLNMbXmeNRVecPx+vXr0Wg0lp8zZ840akOySqWqVE0DUFZWds37rv6bq8vxqcljjz3G+vXrSU9P58MPP2TAgAFERETUrzASIJNBm7Z79258fHxYsGABkZGRXHfddXW+n6CxhIeHA7B3717LMoPBwB9//NGg7fbo0YOdO3dWWrZjxw4cHR3p1KmTZVlERATPPPMMmzZtYtasWbz//vuW13Q6HWPGjGHJkiUcPXqUwsLCatsyTpw4QVpaGq+99hrDhw+ne/fuZGVlXXPirE14eDjp6emcOXPGsiw9Pb3SVUtVvvjiCzQaDYcOHar0s337do4cOcJvv/2Gn58fwcHB/PLLL9XuW6/XV/s6gJ+fH4mJiZWWHTx4sNZyWXN8+vTpw/Hjx2v8Wxw5ciShoaGsXLmS1atXy6uCRqCxdQCS7XTt2pW0tDRWrVrFiBEj2L17N++++65NYunSpQu33XYbs2fPZuXKlfj6+rJ06VJyc3Otulq4cOEChw4dqrQsMDCQuXPnctttt7F48WLuvPNODh06xPz583n22WdxcHAgNjaWDz74gNtuu42QkBASExPZtWuXpbF01apVmEwmbrrpJjw8PNiyZQt5eXmW5HW19u3bo9PpeOedd3j22Wc5f/48c+bMqfMVz6hRo7jhhhuYMmUK77zzDg4ODrzwwgtotdoa11u5ciV33HEH119//TWv9e/fn5UrVxIZGcm8efN4/PHH8ff3Z9KkSZhMJrZt28a9996Lj48Pzz77LPPnz8fR0ZGbb76ZoqIifvzxR+bOnQtAVFQU7777LnfccQft27fnvffeIz4+3tKTrTrWHJ/77ruPJUuWMGHCBJYsWUKnTp04d+4c6enp3HPPPYD5KuqRRx7hpZdewtHR0bJcagAbt1lITaC6BuSqGllfeukl4efnJ5ycnMTYsWPF559/LgARFxdX5baq2rYQQqjVavHxxx9Xu7+q9j9q1Cjx4IMPWp6np6eLu+66Szg6OgpfX1/x8ssvi0mTJonx48fXWF6gyp9FixYJIYT45JNPRLdu3YRWqxWBgYHixRdfFGVlZUIIIRITE8Udd9whgoKChIODg2jXrp146KGHLI2t69atEwMGDBAeHh7C0dFR9OjRQ3z44Yc1xvO///1PdO7cWeh0OtGrVy+xffv2SsenvAF5165dldbr1KmTmDdvnuV5XFycuPnmm4VOpxNBQUFi+fLlYtiwYdU2IB88ePCahvyKli9fXqkh+b///a/o2bOncHBwEF5eXuLWW28VWVlZQgghTCaTWL58ubjuuuuEVqsVfn5+YtKkSZZt5ebmiilTpggPDw/h6+sr5s2bV2UDclWx1nZ8hBAiKSlJTJ06VXh7ewudTie6du1a6XUhhEhLSxNarVY88cQTVZZXqhtFCDnTmdQyGY1GunXrxoQJE1i6dKmtw5FamGPHjhEREcGhQ4e44YYbbB1OqyeriaQWY+fOnaSmpnLjjTeSl5fHsmXLOH/+PNOnT7d1aFILUlJSQnp6OnPnzmXEiBEyETQSmQykFsNoNLJgwQJiY2PRarVERESwbdu2Kuu/pbbriy++YObMmfTo0YOvv/7a1uHYDVlNJEmSJMmupZIkSZJMBpIkSRKttM3g6ptdrOXj40N6enojR2Nb9lYmeysP2F+Z7K08YH9lqqo8tc1JIq8MJEmSJJkMJEmSJJkMJEmSJGQykCRJkpDJQJIkSUImA0mSJAmZDCRJkiRkMpAkqZUoMwo2nc6iqKxhs8FJVZPJQJKkVuF/x9J570AKP53JsnUodkkmA0mSWrzzWcWsO5YBwLZzuXWeRrS1O5NRxFt7E8kuMjTZPpplOIrS0lLmzZuHwWDAaDTSv39/Jk+ezIoVKzh+/DhOTk4AzJ49mw4dOjRHSJIkNTEhRJ2n+6yK0ST492/JOGnVjO/qyedH0onLKiHMS98IUbZcZUYTv17I44dTWZzOKEavUTE41I0+QS5Nsr9mSQZarZZ58+ah1+sxGAy88sor9OrVC4CpU6fSv3//5ghDkqRmkpRXyt+jLzCtly/DO7o3aFsbT2VxJqOYZwcFcmM7Z776M4OtcTl2mwwyCsv46Uw2v8Rmk11sJNDVgYf7+jEyzB0nrbrJ9tssyUBRFPR68wdnNBoxGo2N8o1BkqSWaVtcDhmFBt7Zl0w7Vwe6+jjWazvJeaX893Aa/YKcGdLeFUVR6Bfkws64XKbf6IdGZR/nESEEJ9OK2Hg6i70X8jAJ6BPozLiunvRq54yqGc6XzTa5jclk4oUXXiA5OZlbbrmFKVOmsGLFCk6fPm2Z1eqBBx5Aq9Ves250dDTR0dEALF68mNLS0nrFoNFoMBiars7NFuytTPZWHrC/MtVWHiEE930Wg6tOTXaxgZIyIx/e2ws/V12d9iOE4K/f/snxlHzWTOltWX/XuQzmbDjBktvCGRTm1aCylLPVZ1RcZmTL6XS+PpzI6bQCXBzUjOvhz5092xHsUb8EClWXx8HBocZ1mn2ms4KCAt58801mzJiBq6srHh4eGAwGVq5cSUBAAJMmTap1G3II6yvsrUz2Vh6wvzLVVp6zmcU8s+k8syMD6ObjyPM/xxPo5sCim0PRaazvsxJ9Npt39iXzWD9/xl7naVluMAlmfBNLhL8TLwwJalBZyjXXZ5RXYuREWiHHU4s4nlbE2cwiDCYIdXfg1us8Gd7RHUdtw/v11GcI62afz8DZ2ZkePXpw6NAhJkyYAJjbFEaMGMGGDRuaOxxJkhrZzvO5qBXoH+KKm07NM4PasXDHJd7Zl8SzgwKtqiLOLDLwUUwqPfwcuaWLR6XXNCqFoR3c+OlMNvklRlx0TVeP3lBpBWUcSy3kRFoRx1MLuZBjrtXQqKCzlyMTunnRO9CZCD8nm1edN0syyM3NRa1W4+zsTGlpKUeOHOH2228nKysLT09PhBAcOHCAkJCQ5ghHkqQmYhKCXfG53NjOGbfLJ+mbgl2Z0suX1YfSaO+Rwd0RPrVu5/0DyZQaBLMj21VZXz6iozsbT2Wx+0IuY7p4VrEF27mYU8LXxzI4mlJIeqG5qsZRo6KbryNDOrjRw9eJzt76Ol0lNYdmSQZZWVmsWLECk8mEEIIBAwbQp08f/vGPf5CbmwtA+/bteeSRR5ojHEmSmsiJtCIyCg1M6+Vbafld4V7EZ5fw38PphLrriAxxrXYbey7ksjchn2m9fAlyq7qeu5OXjlB3B7aeaznJoNhg4quj6Xx3MhMHtYob2zlzh58j4b5OtPfQoW7hjd3Nkgzat2/PkiVLrlk+b9685ti9JEnNZNf5XBzUCpHBlU/2iqLwf5EBJOWV8q89Sbw+WksHz2u7huaXGFl5IIUwTx0Tu1ffOKwoCiM6uvPpoTQSc0sJrCZpNAchBPsu5rPq9xTSCg2MDHPjwRv98NC3rlmFW9Z1iiRJrZbRJPj1Qh79glyqbATVaVTMHRqEo1bFazsukVt8be+dj2JSyS0x8mT/drV+kx7W0Q2VYu7GaitJeaW8uv0ii3dewslBzaKbQ/nLgMBWlwhAJgNJqrPfL+Xz3YlMW4fR4hxOLiC3xMjQDm7VvsfbScuLQ4PIKjLw+q5LGExXOjMeSipgy7kc7ujuZdUNZd5OWnoGOLM9LgdTMw9PUWIw8fmRNJ7cGMfx1CJm9fFj2dgOhPs5NWscjUkmA0mqo2+OZ/BRTCp/phTaOpQWZVd8Ls5aFX0CnWt833U+jvxf/wD+TC3ig99TAHN9+4rfkgl0deCe62tvYC43oqMbqQUGjqcWNSj2ujhwMZ8nf4hj7dEMBoS6suK2jkzo5tXi2wRq0/quZSTJhkxCcDazBICVB5JZdmtHu7kLtiFKjSb2JeTTP8QVrbr275jDO7oTn13CN8czae+hIymvlNSCMhZG1e1ehAEhrvxHk8LWczlE+Dftt/LU/DI++COF/RfzCXZz4NVRIfQMqDnxtSYyGUhSHSTmllJsMNE/xIV9CflsPJXJxO7etg7L5v64VEBhmanGKqKrTbnBlwvZJXzwewpCwNguHvSo4wldp1ExKNSVXy/k8Wg//ybprimEIPpsDh/+kQoIHuzly23dvNCq7etLgKwmkqQ6iM0sBuC+633oG+jMF0cyyCgss3FUtrczPhd3vZqedTiZq1UKzw4OJMjNAR8nDdNu9K19pSqMCHOj2GBiX0JevdavSVaRgdd2XOTfvyXTxVvPO+PCuLOHt90lApDJQJLq5GxmMQ5qhRB3HQ/19cdoEnwck2rrsGyqsMzI75fyGRTqWud6cyetmqVjOrD81o71HpGzh58Tfs4atsbl1mv96uy5kMuTP8RxOLmQh/r48c9RIfi5XDt2mr2QyUCS6uBsZjEdPc03ELVzdeCuHl7sis/jcHKBrUOzmf0X8yk1Coa2t76KqCKdRtWgISVUisLwju4cSS5olKu0/FIjy35N5PVdifg5a/nX2A7c1s2rWUYOtSWZDCTJSuWNx50rdHu8M9ybABct7x9IoczYtmbfKrfzfC6+Thq6+tZ/lM2GGtHRHZOAHQ28OjiUVMBTP8SxMz6X+673Yckt7Qlxr9toq62VTAaSZKXyxuNOFZKBTqPi4b7+XMwtZcPJtnfvQW6JkUNJBQzp4GbTb86BbuY5E7bG5dRrSsziMiPv/57CvK0JOGpULLmlPff29GlTPcVkMpAkK5U3Hne66oaovkEuRAa78OXRdNIK2lZj8p4LuRgFDKlnFVFjGtHRjYScUs5lldRpvdPpRUz//BA/nMritm6e/GtsB7p42+4qx1ZkMpAkK8VWaDy+2qw+fgjMwym0JbvO5xLk5kBHT9tXpQxp74ZGpbD1nHXDU+QWG3hvfzIv/BJPqdHEq6NCeKhP03RPbQ3aZqklqR7OZhTT0VNfZY8ZfxcH7u7hzZ4LeRxMahuNyRmFZRxLLWJoezebj8UP4KJTc1OwCzvP51Ya5uJqBpNgw8lMHttwjp9jsxnbxYPPHrjRrm4gqw+ZDCTJCkaT4FxWCZ29qv8GfEe4F+1ctbx/IJkyo6kZo7ON3fF5CGBIHW40a2ojO7qTW2IkJjG/ytf/uJTPX36I48M/Uuni7chbt3bkkX4BuOjk/bcyGUiSFRLzrm08vppWreKRvv4k5pWxvg0MZLcrPpdOXrpq5xywhRsDnXHXqdl6rnKvoos5JfxzWwL/3H4RkxC8NCyY+SOCCfWwffVWSyHToSRZ4ezlxuPOtTQs9g50YUCIK1/9mcGwDu52e5NSUl4pZzKKmV7Pu4abSvmUmJvOZJNXYkQBvjyazo+ns9BrVMzs7cet13na5R3EDSWTgSRZobzxONiKb8Gz+vgRk5jPh3+k8OKw4GaIrvntOm/+5j24BfQiutrIMHc2nMri378lcSy1iIJSIzd38uCBG3xwb4XzDDQXWU0kSVaoqfH4ar7OWiZf78NvF/P5/VLVddetmRCCnfG5hPs64uvc8q58OnrqaO+uY19CPh08dCwb24EnIgNkIqiFPDqSVAtz43Exo8LcrV7n9m5ebD2Xwwe/pzCiR2gTRtf8zqYXkpBTyqP9/G0dSpUUReFvgwPJKDLQK8CpRfR0ag2aJRmUlpYyb948DAYDRqOR/v37M3nyZFJTU1m+fDl5eXmEhYXx5JNPotHI/CS1LObGY1Fre0FFWrXCI339mbc1gU0nUhjcrnG+QecUm2cIG97RnZs7udvkRBd9Og2VAoNCq5/U3tZCPXSycbiOmuXMq9VqmTdvHnq9HoPBwCuvvEKvXr3YuHEj48aNY9CgQbz//vts3bqV0aNHN0dIkmS1s9XceVybGwKcCHZzYPOpNAa3C2yUWHacz+VYahHHUos4mlzI45H+9R7tsz6EEGw5ncYNAc6y2sXONEubgaIo6PXmfySj0YjRaERRFI4dO0b//v0BGD58OAcOHGiOcCSpTmIzrG88rkhRFIZ1cOPQpdxGG6ZiR1wuYZ46HrjBh90Xcnlm03nOXU5WzeF0RjGJuSV1msRGah2aLbWbTCZeeOEFkpOTueWWW/D398fJyQm12vytxsvLi8zMqvtmR0dHEx0dDcDixYvx8bF+jtSKNBpNvddtqeytTC2xPPF5iXT1c8Hfr+7dKCfc6MyaI+nEpBt5oH27BsVxIauI2MxinhzSkXt7BzGwSw7zNp3i+V/ieWpIGHf0DKhztZFJCPadz+Knk6kUlhoxCYFJgMkkMGH+LQQYhUAIQUZBGQ5qhXE3tMfZjm7Uaol/dw1Rn/I026epUql44403KCgo4M033yQxMdHqdaOiooiKirI8T09Pr1cMPj4+9V63pbK3MrW08hhNgtOpeYzq5FGvuPRAeIArm44lcUv7ulUzXW39kTQU4EYfFenp6QTr4F9jQnlrbxJLt59l77lUZkcG4OJQe7VRscHE1nM5bDiZRWJeKZ56Nd5OWlSK+YpGpVDpsRZQVArBbhom3xhIUV42RY0/sZjNtLS/u4aqqjyBgTVXVTZ7and2dqZHjx6cPn2awsJCjEYjarWazMxMvLy8mjscSarRpfLG4zq2F1Q0uqsvy3ec40JOCaH1HBtfCMGOuFyuD3DC2+lKY7S7XsNLw4NZfyKT/x5K42xmMc8NDqx21M20gjJ+PJ3Fz7HZFJSa6OKt59lBgQwMdbV6uGZ7O3FKZs3SZpCbm0tBgXnwrtLSUo4cOUJQUBA9evRg3759AGzfvp2+ffs2RziSZLWzGfVrPK5oZBcfVMqVG7Xq43RGMcn5ZQyvoq5epSjcGe7NwpvbYzIJ5vwSz/cnMyuN638qvYg3d1/ike/Osv5EJr0CnFk8OpQ3bmnP0A5ubWrcfqlqzXJlkJWVxYoVKzCZTAghGDBgAH369CE4OJjly5fz5Zdf0rFjR0aOHNkc4UiS1c5mFqOrR+NxRd7ODvT0d2Ln+Vzu7+lTr+6gO+JycFArDKihO2c3X0eW3dqRd/YlseqPVI6mFDI41JUfTmdzKr0IJ62KCd28GHedp90OkyHVX7Mkg/bt27NkyZJrlvv7+7No0aLmCEGS6iU20/o7j2sytIMbb+9L5nRGMV196jZxisEk2B2fR78gl1q7kbrq1MwdGsTGU1l8cjCV/RfzCXDR8nBfP0aGuTdrN1SpdbGf7gCS1MiMJsG5zGKiOns0eFv9Q1z5z/4Udp7PrXMyOJxUQE6JkWFWdudUFIXbunnRq50zGYUGrvd3anAyk+yfHJtIkqpxKa+UEmPDGo/LOTuo6Rvkwu74XIw1TLxSle3nc3FxUNE70KVO64W46+jVzlkmAskqMhlIUjXKG48bIxkADOvgRnaxkaMphVavU1Rm4reEPAaFuslhl6UmJZOBJFUj9nLjcWNN3tInyBknrYoddehV9NvFPEqMgmEd5R2/UtOSyUCSqnE2s5gwr4Y3HpdzUKsYEOLK3gt5lBismxZz5/lcfJ00dPetWzuDJNWVTAaSVIXyxuOG3F9QlaEd3CgymPijmjl6K8ouNnAwqYChHdxQyWGYpSYmk4EkVeFSrrnxuLGTwfX+Tnjo1ey0oqpod3wuJgHDOlo/j4Ik1ZdMBpJUhdjMxm08LqdWKQxp78bvlwrILzXW+N4dcbl08NDRXo7LLzUDmQwkqQpnG7nxuKKhHdwoMwn2JVQ/0ltSXimnM4plw7HUbGQykKQqxGY0buNxRV289QS4aGusKtpxPhcF5LwBUrORyUCSrmI0CeKyGr/xuJyiKAzt4MbRlEIyiwzXvG4eoTSHHv5O+DjJMYSk5iGTgSRdpbzxuLHbCyoa2sENk4Bf46+9OojNLCYxr8zq4SckqTHIZCBJVylvPO7k3XTJIMRdR5inrsob0HbE5aJRKQxswRPOS/ZHDlQntQhCCN47kMKWc6dw0qpwdVDj4qDGVafCxUGNi05dYZkaN52aHn5OTTJEQ2xmMXqNQpBr4zceVzS0gxufHEwjKa+Udpf3ZTQJdsXn0i/I2aoZyySpschkILUIG09l8dOZbEZ09sYBI/mlRvJKjWQUGojPLiGvxETRVXfthrg78MRNAYT7OTVqLGczGmfY6toM6eDGpwfT2Hk+l3uuN89XeySlkOxiI8M6yHsLpOYlk4Fkc3+mFPJRTCqRwS7889ZuZGZkVPk+g0mQX2okv8TI+ewSPolJZe7mC9zS2YNpvXxx0TX8m3R54/HoRhi2ujY+Tlp6+Dmy43wukyO8URSF7XE5OGtV9AlybvL9S1JFss1Asqn0wjKW7L5EO1cH/jqwXY3DLmhUCh56DcHuOga3d+Od8WFM7O7F5rPZPLHxHDvP51aa6rE+LjbRncfVGdrBnUu5pcRllVBiMLEvIZ8Boa44qOW/ptS85F+cZDNlRhOLd16ixCCYOzSozrNwOWpVzOjtx9IxHfBz1rL010T+se0iyXml9Y7pbDM0Hlc0INQVjcp8X8FvF/MpNphkLyLJJpqlmig9PZ0VK1aQnZ2NoihERUVx66238tVXX7Flyxbc3Mx//Pfddx+9e/dujpCkFmDlgRTOZBQzZ2gQIe71H3IhzEvP66Pbs+lMFv89lM6TP8Rxz/U+TOzuVeeJ3pur8bicm07Nje1c2BWfS0JOCd6OGiL8G7cNRJKs0SzJQK1WM3XqVMLCwigqKmLOnDn07NkTgHHjxjFhwoTmCENqQX4+k83mszlM6uHNgJCGd6FUqxTGd/Wif4grH/6ewupDaeyMy+WJyAC61WH459iMYsKaofG4oqEd3DhwKZ+MQgN3dPeSI5RKNtEs1USenp6EhYUB4OjoSFBQEJmZmc2xa6mJGEyCojLrxuS/2qn0It7/PZkb2zlzf0+fRo3Lx0nLnKHBvDgsiPwyI3N+ieff+5JILyyrdd2mvvO4OjcFu6C73EVWjkUk2YrVVwaffPIJw4cPp0OHDg3aYWpqKnFxcXTu3JmTJ0/y888/s3PnTsLCwpg2bRouLtfO8xodHU10dDQAixcvxsenficQjUZT73VbKluV6dWfT7H1TAZ33dCOB/oE42nlsAkZBaUs2X0OP1c9CydE4KavvF5jlWecjw/Dw0NYte8C6w4nseN8LrdfH8DUviF4O1ddBXQuvYBSo6BXe99GPabWlGlM91xi0wvo2zkIpYVfGcj/o5avPuVRhJXdLz766CP27t2Lm5sbQ4YMYciQIXh7e9dpZ8XFxcybN48777yTyMhIsrOzLe0Fa9euJSsriyeeeKLW7SQmJtZpv+V8fHxIT0+v17otlS3KdDG3hP/bEEeQmwOJeaU4qM1VNHd096qxe6fBJHg5+gKxmcUsuaU9HT2v/QbeFOVJzS9j7Z/pbD2Xg0alcOt1ntwZ7oW7vvJ3oS1ns3l7XzL/Ht+xQW0YV7OmTEIIBLSKKiL5f9TyVVWewMDAGtex+spg5syZTJ8+nYMHD7Jr1y6++eYbunTpwtChQ4mMjESvr/nS2mAwsHTpUoYMGUJkZCQAHh4eltdHjRrF66+/bm04kg2tO5aBVq3w2s2h5JUY+eJIOl8fy2DT6SwmdPdiQjfPKnsGfRSTyvG0Ip4dFFhlImgqfi5anuzfjkk9vPnyaDrfn8zkpzPZjO/qWSmBnb3ceBzYTI3HFSmKQstPA5I9q1MDskqlok+fPvTp04eEhATefvtt3n33XT788EMGDRrE5MmT8fLyumY9IQTvvfceQUFBjB8/3rI8KysLT09PAPbv309ISEgDiyM1tZT8UrbH5TLuOk889Bo89BqeHxLE+axiPj+SzhdH0tl4MpM7wr0Z19UTvcbcLLXtXA4/nMpiQjdPmw3L3M7VgacHBjKph3eVCSw2s6TZG48lqaWoUzIoLCxk37597Nq1i/j4eCIjI5k1axY+Pj5s3LiRhQsX8uabb16z3qlTp9i5cyehoaE899xzgLkb6a+//sr58+dRFAVfX18eeeSRximVVEl6YRnZRUY6N0Lf+W+PZ6JSYGJ45aTfwVPPi8OCOZNRxOeH0/nsUBrfncxkUg9vrvN25N39yUT4OzH9Rr8Gx9BQIe66KhNYkUEw9joPW4cnSTZhdTJYunQphw8fpnv37tx8883069cPrfZK49+0adOYPn16let269aNr7766prl8p6CppdRWMbzP8eTW2zk7XEdCWzAzF0ZhWVsPpvDqDCPasfZ7+LtyLyRIZxILWTNkXRW/ZEKgLeThucGB7aob90VE9gXR9L5I7GA6xt5nCNJai2sTgZdunRh1qxZler5K1KpVHzwwQeNFZfUCArLjLy6/SIFpSa0aoWVB5KZPzKk3r1VvjuRiUkI7gy/tirwat39nFgQFcqR5AJ+ic3mjnBvPPQtcyisLt6OvDIihOxiA+6NML6RJLVGVv939uzZE4Oh8qxM6enp5OfnW7qb6nRy4u6WwmASvL4rkfjsEl4eHkxSXhnv/57Crvi8etXZ5xYb+OlMNkPbuxFQhwbWngHO9AxoHYOutdRkJUnNweqbzt555x2MRmOlZQaDgX//+9+NHpTUMEII/rM/mUNJBTxxUwC9A10Y08WDzl56PvojhfxSY+0bucr3J7MoNQruiqhbd2JJkloHq5NBeno6/v7+lZYFBASQlpbW6EFJDfO/PzOIPpvD5Ahvbr48FLNapfD4TQHklBhZc7hun1l+qZEfTmfRP8SV0Ebsfy9JUsthdTLw8vLi3LlzlZadO3fO0jVUahm2nsthzZF0hnd0u2aoh87eesZe58mm09mcySiyeps/ns6isMzEZHlVIEl2y+pK0nHjxvHGG28wYcIE/P39SUlJYcOGDdx5551NGZ9UB4eTC/j3viR6+jvxf5HtqmwofqCnD3su5PHub8m8OaZDrb17ig0mvj+ZRZ9AZ8KaecweSZKaj9XJICoqCmdnZ7Zu3UpGRgbe3t5MmzaN/v37N2V8kpXis0tYvPMSQW4OvDA0qNq5gZ0d1DzUx483dify4+ksbutWc8+gn89kk1diZHKE/YzbIknSterUfWLAgAEMGDCgqWKR6imjsIx/bktAp1HxyoiQWidSHxTqSnQ7Z9YcTmdgqCve1dwzUGo08e2JTK73d6rTMNCSJLU+dUoG2dnZxMbGkpeXV2l6wZEjRzZ6YJJ1CkoNvLr9IvmlJhbdHIqvc+2jhyqKwqP9/HlyYxyr/kjl+SFBVb5vy9kcsooMPDOwXWOHLUlSC2N1Mti/fz/vvPMO7dq1IyEhgZCQEBISEujWrZtMBjZiMAle/vGk5V6CutTpt3N1YHKEN2uOpBOTmE/vwMpDhxtMgm+OZ9DVR8/1cuYtSbJ7VieDtWvX8sQTTzBgwABmzJjBkiVL2LZtGwkJCU0ZX5uTW2LkRGohJgEmBEKASVwZ4tgkwCTMyw8lF/BbfB6zIwOuOZlb445wL7afz2XlgRTeHueETnOlc9nO87mkFhh4pG9Aix9fX5KkhrM6GaSnp1/TXjBs2DAeeeQRpk2b1uiBtVVLf03kUFKB1e+fcVMIozvX7w5frVrFY/38eXlLAl8fy+CBG3wB84xf//szg46eOvoGtY67hyVJahirk4GbmxvZ2dl4eHjg6+vL6dOncXV1xWSq39SH0rX+TCnkUFIBkyO8GRjqikpRUBTzzSCKoqBSQKWAgnm5g1qhU3BAgybl6BngzPCObnxzPINhHdwIdtexNyGPxLxSnh8cKK8KJKmNsDoZjBo1ipMnT9K/f3/GjRvHP/7xDxRFqTQ/gVR/Qgj+ezgNL0cNk3p4V6qyaWozevtx4FI+/zmQwqujQvjfnxkEuznQvxEmqpckqXWwOhlMmDABlcp8gho2bBg9evSguLiY4ODgJguuLfkjsYATaUU81s+/WRMBmAdoe7CXH+/uT2bZniTOZ5fwlwHtWtRw05IkNS2rzjomk4mpU6dSVlZmWebj4yMTQSMxCcGaw2kEuGiJ6uRhkxhu7uxOVx89O8/n4u+itdlsZJIk2YZVyUClUhEYGEheXl5Tx9Mm7U3I41xWCfde71PtncNNTaWYB7Jz1qq473ofNPKqQJLaFKuriQYPHszrr7/O2LFj8fb2rtSwGBER0STBtQVGk+Dzw+mEuDvY/Nt4R089n97VxWYJSZIk27E6Gfzyyy8A/O9//6u0XFEUOadBA+w4n8vF3FJeGNIypoSUiUCS2iark8GKFSvqvZP09HRWrFhBdnY2iqIQFRXFrbfeSn5+PsuWLSMtLQ1fX1+efvppXFzqfvNUa1VmFHx5NJ1OXjoGyJ47kiTZULPM86dWq5k6dSphYWEUFRUxZ84cevbsyfbt27n++uuZOHEi69evZ/369UyZMqU5QmoRos9mk5JfxqPDg2V/fkmSbMrqZPD4449X+9p//vOfGtf19PS0TILj6OhIUFAQmZmZHDhwgPnz5wPm7qrz589vM8mgxGDiqz8zCPd1pHegvMtXkiTbsjoZPPnkk5WeZ2Vl8eOPPzJo0KA67TA1NZW4uDg6d+5MTk6OJUl4eHiQk5NT5TrR0dFER0cDsHjxYnx86je2vkajqfe6je2LmItkFhl4dVx3fH3d672dllSmxmBv5QH7K5O9lQfsr0z1KY/VySA8PPyaZT169OC1117j1ltvtWobxcXFLF26lOnTp+PkVHkkTEVRqq0qiYqKIioqyvK8vsMv+Pj4NGjohsZSWGbks/0J9GrnTLCurEExtZQyNRZ7Kw/YX5nsrTxgf2WqqjyBgYE1rtOgW101Gg2pqalWvddgMLB06VKGDBlCZGQkAO7u7mRlZQHmKw03t7Zxo9OGk1nklhiZcoP9fBORJKl1q9MQ1hWVlJRw8OBBbrzxxlrXFULw3nvvERQUVGkso759+7Jjxw4mTpzIjh076NevXx1Cr5vCMiOnU/Pxat6RHq6RV2Jk/YlMIoNd6OItZw+TJKllsDoZZGRkVHqu0+kYP348Q4cOrXXdU6dOsXPnTkJDQ3nuuecAuO+++5g4cSLLli1j69atlq6lTeW9/Sn8nljAP0YG2/Qk/M3xDIrKTJbhoiVJkloCRVScv7KVSExMrPM6aQVlvLz1InnFZSyICqWjp/WzgjWWrCIDj3x3lgEhrjwzqOb6O2u1hbrO1s7eymRv5QH7K1OTthmsX7+e2NjYSstiY2P57rvv6hCi7fg6a3n7zuvRaVTM25LAhZySZo/hf8cyMJgE9/WUbQWSJLUsVieDH3/88ZpRSoODg/nxxx8bPaimEuiuZ8GoUFQKvBJ9gcTc0mbbd1pBGT+fySaqkzvtXB2abb+SJEnWsDoZGAwGNJrKTQwajYbS0uY7oTaGQDcH/hkViknAS1sukJLfPPF/edR8yTY5Ql4VSJLU8lidDMLCwvj5558rLfvll18ICwtr9KCaWqi7jn+MCqHEYOKl6ATSCspqX6kBfr2Qy9ZzOYzt4oGvs7ZJ9yVJklQfVvcmevDBB1mwYAE7d+7E39+flJQUsrOzefnll5syvibT0VPP/JEhvLIlgZe3XGDhze3xcmz8oZq2x+Xw1t4kuvo4cr+8r0CSpBbK6rNfSEgIb731Fn/88QcZGRlERkbSp08f9Prm75XTWLp4OzJvRAjztl7g5egLvHZzKB76xksIv8Rm8+5vyUT4O/H3YcE4am18k4MkSVI1rD47ZWZmYjAYGDRoEBMmTGDQoEEYDAYyMzObMr4m183XkZeHh5BaUMYrWxLILTE2ynZ/OJXFit+SubGdMy8Pl4lAkqSWzeoz1BtvvHHNiT8zM5M333yz0YNqbuXf3BNzS5m/NYH80oYlhG+PZ/D+7ylEBrvw4rCgZp/gXpIkqa6sPkslJiYSGhpaaVloaCiXLl1q9KBsoVc7Z+YMDSI+u5h/bksgu9hQ520IIVh7NJ1PDqYxuL0rzw8JQquWiUCSpJbP6jOVm5sbycnJlZYlJyfj6mo/M3T1DXLhb4ODiM0oZta3Z1n6ayInUgux5iZtIQT/PZzO50fSGdHRjWcGBspJ5SVJajWsbi0dMWIES5cu5d5778Xf35/k5GTWrl3LyJEjmzK+ZjcgxJW3x3Vk05lstp7LYef5XDp46Bh7nQfDOrhXWfcvhGBVTCobTmZxS2cPHrvJH5WcuUySpFbE6mQwceJENBoNq1evJiMjA29vb0aOHMltt93WlPHZRLC7jof7+jO1ly87z+fy4+ks/rM/hU9i0hgR5sbYLp6EeugAMAnBygMp/HQmm/FdPXmoj5+cwlKSpFbH6mSgUqmYMGECEyZMsCwzmUwcPHiQ3r17N0lwtqbXqBjd2YObO7lzKr2YTWey+CU2hx9PZxPh58iYLp7EJBWw9VwOd4Z7Ma2Xr0wEkiS1SvXqVB8fH8+OHTvYvXs3RqORVatWNXZcLYqiKHTzdaSbryMzexvYcjaHn2KzefNX8+ip9/X04Z4Ib5kIJElqtaxOBjk5OezatYudO3cSHx+PoijMmDGDESNGNGV8LY67XsOdPby5vbsXB5MKKDGYGNS+bczQJkmS/ao1Gezdu5cdO3Zw+PBhgoKCGDx4MM899xx///vf6d+/Pw4ObXMETrVKoW+Qi63DkCRJahS1JoPly5fj4uLC008/zU033dQcMUmSJEnNrNZk8Pjjj7Njxw7+9a9/0alTJwYPHszAgQNl/bgkSZIdqTUZDB8+nOHDh5OWlsaOHTv46aef+OyzzwA4ePAgQ4cORaWq+d61d999l5iYGNzd3Vm6dCkAX331FVu2bMHNzVzfft9999ltryRJkqSWzuoGZF9fXyZNmsSkSZM4efIkO3bs4NNPP+WLL75g5cqVNa47fPhwxowZw4oVKyotHzduXKWuqpIkSZJt1JoMjhw5Qnh4eKVZzrp160a3bt2YOXMmBw4cqHUn4eHhpKamNixSSZIkqcnUmgw2bNjAW2+9RdeuXenduze9e/fGy8sLAK1Wy8CBA+u9859//pmdO3cSFhbGtGnTcHGRvXMkSZJsQRFWjMJWUlLC0aNHOXjwIAcPHsTZ2Zkbb7yR3r17c91119XaZgCQmprK66+/bmkzyM7OtrQXrF27lqysLJ544okq142OjiY6OhqAxYsX13veZY1Gg8FQ99FIWzJ7K5O9lQfsr0z2Vh6wvzJVVZ7abgOwqs1Ap9PRt29f+vbtC8CFCxc4ePAgX375JZcuXaJHjx6MGzeOLl26WB2sh4eH5fGoUaN4/fXXq31vVFQUUVFRlufp6elW76ciHx+feq/bUtlbmeytPGB/ZbK38oD9lamq8gQGBta4Tr2GowgNDSU0NJTbb7+dwsJCDh8+TFFRUZ22kZWVhaenJwD79+8nJCSkPqFIkiRJjcDqZPDnn3/i5+eHn58fWVlZrFmzBpVKxf3338+AAQNqXHf58uUcP36cvLw8HnvsMSZPnsyxY8c4f/48iqLg6+vLI4880uDCSJIkSfVjdTJYtWoVf//73wEs9xmo1WpWrlzJCy+8UOO6f/3rX69ZZm/zIEiSJLVmVieDzMxMfHx8MBqNHD58mHfffReNRsOjjz7alPE1KmtmLJMkSWqLrJ720tHRkezsbI4fP05wcDB6vR6g1bTAmzZ8SdYcWRUlSZJUFauvDMaMGcPcuXMxGAxMnz4dgJMnTxIUFNRUsTUuBx1lp4+hys5A8fC2dTSSJEktSp2mvbzppptQqVQEBAQA4OXlxWOPPdZkwTUmpXtPBCBOHkHp37bmYJAkSaqN1dVEYO6nWp4I/vzzT7KzswkNDW2SwBpdcEcUV3c4ccTWkUiSJLU4VieDefPmcfLkSQDWr1/PW2+9xVtvvcU333zTZME1JkWlwiGiN+LkYdmQLEmSdBWrk0FCQgLXXXcdAFu2bGHevHm89tprbN68ucmCa2wOPftCZjqkJtk6FEmSpBbF6jaD8m/TycnJAAQHBwNQUFDQBGE1DYee5uE0xInDKP4135otSZLUllidDLp27cpHH31EVlYW/fr1A8yJwdXVtcmCa2zqdsHg5YM4eRiGj7V1OJIkSS2G1dVEs2fPxsnJifbt2zN58mQAEhMTufXWW5ssuMamKApKtxvg5FGEyWTrcCRJkloMq68MXF1duf/++ysta5XTVHbvCXu2QEIctO9k62gkSZJaBKuTgcFg4JtvvmHnzp2WEUeHDh3KnXfeWWkWtJZO6VZ+v8FhFJkMJEmSgDokg//+97+cPXuWhx9+GF9fX9LS0li3bh2FhYWWO5JbA8XDG9qFIE4chlvutHU4kiRJLYLVbQb79u3j+eef54YbbiAwMJAbbriBv/3tb+zdu7cp42sSSreecOY4wlBm61AkSZJaBKuTgT3dqKV0vwFKS+DcKVuHIkmS1CJYXU00YMAAXn/9dSZNmmSZUm3dunW1TmzTHIQQFBcXYzKZUBSl2velpKRQUlKCCOuOuO8xEKAqLGzGSBtfeZkagxAClUqFXq+v8ThKkmR/rE4GU6ZMYd26daxatYqsrCy8vLwYOHBgixjCuri4GK1WW2tDtkajQa1WAyC6Xw8oKE5OzRBh06lYpsZgMBgoLi7G0dGx0bYpSVLLZ3Uy0Gg03HPPPdxzzz2WZaWlpUydOpUpU6Y0SXDWMplMde/RpHeE3GyEyYSiqtN4fXZNo9E02pWGJEmtR4POgi2lKqFeceidQAgoLmr8gFq5lvK5SpLUfJrlBoF3332XmJgY3N3dWbp0KQD5+fksW7aMtLQ0fH19efrpp3FxcWmOcMx0elAUKC4EJ+fm268kSVILVGsy+PPPP6t9zdr2guHDhzNmzBhWrFhhWbZ+/Xquv/56Jk6cyPr161m/fn2zVjcpKhVC5yivDCRJkrAiGfznP/+p8XUfH59adxIeHk5qamqlZQcOHGD+/PkADBs2jPnz5zd/24PeEbIzEEYDirr+F0k5OTl8++23db75burUqfz73//G3d29Tuv99a9/JSoqivHjx9dpPUmSpOrUegas+G2+MeXk5ODp6QmAh4cHOTk51b43Ojqa6OhoABYvXnxNAkpJSbG6Abni+0wurhizM1CXlaLS6etaBIuCggI+++wzHnrooUrLDQZDjXF98cUX9dqfSqVCrVZbtt3Yw4HodDqrknxT0Gg0Ntt3U7G3MtlbecD+ylSf8rSIQYUURamx0TIqKoqoqCjL8/T09Eqvl5SUWLpXmr78AJEQV+1+rrl5rqQIk0oNWofq4wvpiOreh6t9/dVXXyU+Pp4RI0ag1WrR6XS4u7sTGxvL7t27mTlzJomJiZSUlDBr1izLFVBkZCSbNm2ioKCAKVOmcNNNN/H7778TEBDARx99VG33TpPJhNFoxGAwsGfPHubPn4/RaOSGG25g0aJF6HQ6Fi5cyC+//IJGo2Ho0KG88sorbNiwgWXLlqFSqXBzc6t2lrqSkpJrjnFzKb+HxZ7YW5nsrTxgf2WqqjyBgTXP4WKzZODu7m4Z8C4rKws3NzfbBKJSQQOHs37xxRc5deoUmzdvZs+ePUybNo2tW7da5odeunQpnp6eFBUVMW7cOG699Va8vLwqbSMuLo4VK1bwxhtv8Oijj/Ljjz9y11131bjf4uJi/vKXv/Dll1/SqVMnnnrqKT777DPuuusuNm3axM6dO1EUxXLVtXz5ctasWUO7du1qvBKTJKntsVky6Nu3Lzt27GDixIns2LHDMmFOQ9X0DV6j0VzT6C1ysyEzDYI7oGi0jRJDr169LIkA4KOPPmLTpk2AeQ6IuLi4a5JBSEgIERERAPTs2ZOEhIRa93P27FlCQ0Pp1Mk8+urdd9/Np59+yowZM9DpdDz77LOVrqr69u3L008/zW233cbYsXJyH0mSrmiWu62WL1/OSy+9RGJiIo899hhbt25l4sSJHDlyhKeeeoqjR48yceLE5gjlWvrLVTFFjderyKnCXc179uxh165dbNiwgejoaCIiIqq8qUun01keq9VqjEZjvfev0Wj44YcfGDduHNHR0TzwwAMAvP766zz//PMkJiYyduxYMjMz670PSZLsS7NcGfz1r3+tcvkrr7zSHLuvmdYB1Brz/Qau9auqcnZ2Jj8/v8rX8vLycHd3x9HRkdjYWGJiYhoSbSWdOnUiISGBuLg4OnbsyLp16+jfvz8FBQUUFRUxatQo+vXrZxk/6vz58/Tu3ZvevXuzbds2EhMTr7lCkSSpbWoRDci2pCgKQm++30AIUa+7b728vOjXrx8jR45Er9dXasUfPnw4q1evZtiwYXTq1KlRZ4fT6/UsX76cRx991NKAPHXqVLKzs5k5c6Z5UD4hmDdvHgALFiwgLi4OIQSDBw+mR48ejRaLJEmtmyJa4djUiYmJlZ4XFhZWqpqpTlVtBgAiLwcyUiEwFMVBV8WaLVd1ZWoIa49nU7C3Xh1gf2Wyt/KA/ZWpPr2J5AhtYB6nCOTdyJIktVltvpoIQNFqERqtud3AzcPW4Vi8+OKLHDhwoNKyhx56qNLIsZIkSY1BJoNyeicozKt3u0FTWLhwoa1DkCSpjZDVROUcHc03n5XKsfwlSWp7ZDIoZ7nfoHVPgylJklQfMhlcpqg14KCTjciSJLVJMhlUpHeEkiJEA8cqkiRJam1kMqiofCrMkuIm3U2XLl2qfS0hIYGRI0c26f4lSZKuJpNBRfoKU2FKkiS1IXbXtfTD31OIy6r6m32V8xlcRZSagEwUhwLLso6eeh7q61/tOgsXLiQwMNAy09nSpUtRq9Xs2bOHnJwcDAYDzz//PLfcckudylJcXMzcuXM5cuQIarWaefPmMWjQIE6dOsUzzzxDaWkpQgjef/99AgICePTRR0lKSsJkMvGXv/yF22+/vU77kySp7bK7ZNBgKjUYyhDCfJFgjQkTJjBv3jxLMtiwYQNr1qxh1qxZuLq6kpmZyW233cbo0aPrdA/DJ598gqIobNmyhdjYWO677z527drF6tWrmTVrFnfeeScmk4mSkhK2bt1KQEAAq1evBiA3N7euJZckqQ2zu2RQ0zd4a8bxEcVFkHwRfLxQXFyt2mdERATp6ekkJyeTkZGBu7s7fn5+zJ8/n99++w1FUUhOTiYtLQ0/Pz+ry3LgwAFmzJgBQOfOnQkODubcuXP06dOHt99+m6SkJG677TZCQ0Pp1q0b//znP3nttdeIiooiMjLS6v1IkiTJNoOr6XTmLqaZqYgq5h2ozvjx4/nhhx/4/vvvmTBhAt988w0ZGRls2rSJzZs34+PjU+U8BvVxxx138PHHH6PX67n//vvZvXs3nTp14qeffqJbt24sWbKEZcuWNcq+JElqG2QyuIqiqMCvnbm6KDURUVZm1XoTJkzgu+++44cffmD8+PHk5eXh4+ODVqvl119/5eLFi3WO5aabbuLbb78FzLOaXbp0iU6dOhEfH0/79u2ZNWsWY8aM4cSJEyQnJ+Po6Mhdd93FY489xtGjR+u8P0mS2i67qyZqDIpGi/BrB8mXzAkhIBhFra5xna5du1JQUEBAQAD+/v7ceeedPPjgg4waNYqePXvSuXPnOsfx4IMPMnfuXEaNGoVarWbZsmXodDo2bNjAunXr0Gg0+Pv7M3v2bA4fPsyCBQtQFAWtVsuiRYvqW3xJktogOZ9BDURxEaQkgoMD+AehqFrehZScz6Dls7cy2Vt5wP7KJOczaGSK3hF8/aGkBNKSa+2WKkmS1FrZvJpo9uzZ6PV6VCoVarWaxYsX2zqkShQnF4S3r3kmtIxUhLdfowxxfeLECZ566qlKy3Q6HRs3bmzwtiVJkurK5skAYN68ebi51W8y+uaguLojjAbIzgSNBjy8G7zN7t27s3nz5kaITpIkqeFaRDJoFdy9wGBOCEKtQXF1t3VEkiRJjcbmDcizZ8/GxcUFgJtvvpmoqKhr3hMdHU10dDQAixcvprS0tNLrKSkp6HRNP5G9EAJj8kVEQQHqgCBUVt6U1tqUlJTg71/9zXtNqSkaxG3N3spkb+UB+ytTVeVxcHCocR2bJ4PMzEy8vLzIyclhwYIFzJgxg/Dw8BrXaa7eRFURJhOkXDLPiOYfZG5ktiHZm6jls7cy2Vt5wP7K1Cp7E3l5eQHg7u5Ov379iI2NtXFENVNUKvALBI0WUpMQcppMSZLsgE2TQXFxMUVFRZbHR44cITQ01JYhWUVRq8E/0DySXcolsi9d5JNPPqnzdqZOnUpOTk7jByhJklRHNm1AzsnJ4c033wTAaDQyePBgevXq1aBt/hlTSG62scrXrBnCuipuHmoieleuNlE0WkRAEKSnknv+HJ+tWsWD992HUqHtwmAwoNFUf4jLRxiVJEmyNZsmA39/f9544w1bhtAgitYBERDEolfmc/7SRUaPHo1W54DOyRl3Dw9iY2PZvXs3M2fOJDExkZKSEmbNmsWUKVMAiIyMZNOmTRQUFDBlyhRuuukmfv/9dwICAvjoo49wdKy6PWLNmjWsWbOG0tJSwsLCeOutt3B0dCQtLY05c+YQHx8PwKJFi+jXrx//+9//WLlyJWDu0vrOO+80zwGSJKnVsHkDcn3YsgG5KgkJCTw4bRpbvl7Lnh07ePC5OWzZuIHQbt1RFIWsrCw8PT0pKipi3LhxfP3113h5eVVKBoMGDeLHH38kIiKCRx99lNGjR3PXXXdVub/yRneAN954A29vb2bOnMljjz1Gnz59ePjhhzEajRQUFJCUlMSsWbP4/vvv8fLyssRSE9mA3LjsrUz2Vh6wvzLVpwFZ3mfQWBQFxdsPvH3p1SOcUEctpFxCePny0UcfsWnTJsCcyOLi4iwn83IhISFEREQA0LNnTxISEqrd1alTp1iyZAm5ubkUFBQwbNgwAH799VfeeustANRqNW5ubnz99deMHz/esr/aEoEkSW2TTAaNTNE64OThCd5+kJXBnh++Z9e2rXz/3XqcnF2YNGlSlfMaVLxPQq1WU1xc9dSdAE8//TSrVq2iR48efP311+zevbtJyiJJUtth866l9sDZ2Zn8/PxKyxRXdwhqT57BhLuTI46ZaZw5cpiYmJgG7y8/Px9/f3/KyspYt26dZfngwYP57LPPAHODfG5uLoMGDWLjxo1kZmYCkJWV1eD9S5Jkf+SVQSPw8vKiX79+jBw5Er1ej4+PD2DugjpiwkRWf7ue4fc+QKeQYG6M6GEe56gBnnvuOcaPH4+3tze9e/cmLy8PgH/+8588//zzfPnll6hUKhYtWkTfvn156qmnmDRpEiqVioiICJYvX97QIkuSZGdkA3IzEUJAbrZ5sDsF82B3ru4NHgFV3oHc8tlbmeytPGB/ZWqVdyC3FYqioLh7QmAo6BwhMw2SL8o7mCVJahFkNVEzU7SXp9QsyIesNEhKQLh5gLvXNTOpvfjiixw4cKDSsoceeoh77rmnGSOWJKktkMnABhRFARdXhKMTZKVDThYU5JsnznG8Uj2zcOFCG0YpSVJbIquJbEhRq1F8/CEgyDLOkUhLRhgMcopNSZKalbwyaAEUvROiXQjkZl2+SsgDRYVQq0GtgYq/NZWfC7Xa1uFLkmQHZDJoIRSVCjy8EU6uUFQARiMYDebfZaVQXASmawfgMygqc4LQaECtvfJYc/mxWtMoczZLkmTfZDJoYRQHB6hmRiJhMlVOEkYDKpMJU2mJeVlpvnl5pQ0qCLXGkhgsiaLiMpVKJgxJauNkMrCBLl26cObMmTqvp6hUoFKBVmtZptZoEBXuMxAmk3muZmOZ+bfBAIbLj0uKodAAV7dHqFSXE4Y5SZjSUjCdPQ6OTqB3RNGbf5t/nMDxyuOre0BJktQ62V0y2LlzJ2lpaVW+Vt/5DHx9fRk6dGhDQ2sWikp1+cqimqsLIcxXD4Yy89WEwXDlt8EApcXm+x++W3Nlnep25ugEncNRul2P0rUnhHRAUck2DElqjewuGdjCwoULCQwMZPr06QAsXboUtVrNnj17yMnJoaysjOeee55bRt9iObEajQJEhRPt5Qf5Bfk8/PAscnJyMBjKePaZ5xg9+hYA1q37mg8+WAmKQvdu3Vm27C2Sk5OZO/d5Lly4AMCCVxfSp0/fSvGJCts3UwE6UOuginN3odaX1GeHQVmZub3CUIYoKzM/N5ReeZyXa+79tD8VcWALOOgR3v7g4w/e/ggXd3M1leBKWS2/zQGV5+by33p9qmWQvoo1V5VqsRQFhZq3Y8n5oppkVv6+6lNdo9HpjFcGHhSVP49rHld6UP706jdc9Xo9i2BtzeDV23dwKKW0tLTe22sSFf9WKjyp7thdXSattpSysmvLdLWGdPKzdt2qjmP5sq4Rejy8mua03eaHoxBCIExgElceC3Hlp/w95f/E15zYBBw/cZRFi//JZ598BcD4CaN4f+VqXF1ccXFxJSsrk/semMimH3agKAp9b+rO7/tPVBmjwWCguLjomvXOnj3DU399hDWrv8HT04vsnGw83D149m+zueGG3kybOguj0UhhYQGurm51PKKVpSTnc/pogzYBwoSCMP9bKgooV/6gFctzywLLayqVCpPJZN5E+QG3nPCvHHghyrejoKiubLzCJiv9tlCqfFhhYSOf0YRArVZbymTZvFLhVxWPr4nx2geVt3eVq5dX919e039/ddu2dgiU5juziGofVj6mVwp0ddm02qYdqqYuf1ZVHbfyZRG9HfHyqT0ZyPkMalBaYqKwoBSj0Wg+v5iE+eQvKn47vOprG1z5R738bZQqTjbh4V3JzEwnLf0iWVmZuLu74R/gweJFCznw+35UKhWpqcnk5Cbj6+uLooCDzljhJHD55AiUlRl5819L2L9/PyqVQmpqMgVFqcQc+pVx424lKNg8L4GTiwegsP/AHt56exk6nYKCBndPjythX/NPUJ7UKpxcK/wuP8l6+pTRs39phTJfKbtlGQqKoqBSqVCrFRSVCrVahUqloORkozp3EuXsCZQzx1AyUi2HU1TxXyEcdOYhOvSOqLRaygoLEaXFmEpLMAkFoSiYLv8IrjyuRKc3X4m4uptHjHV1B1c382MXV9A7m6u1tFpLEqqq0by8TIqiXPkpLUaVl4uSlw15OSg5WShGI6LCvoSrG8LFzTwdqhCVjq2HhxuZmZmYTCZMJhNCiCp/gzkZmo+pusLxVVuWq1TmZeX7qPhTvq2KP3VR8bhc/bjic29vV7KzsyvEpLIcM1UtnRGqirs8Tmu3UdX2jEYjRqMRk8lU6bG1vL3dyMnJuaZMFT8La+Mp/13xuDVUc3xnt3kyOHToEB9//DEmk4lRo0YxceLEJtlPQUE+pWVFTbJtgFGjRrLxh/Wkp6cTFTWKdevWkpqWzOrVn6HVahk3bhxZ2ek4OjkghCAvP7vK7Xz//fckJyfx2WefWtZLS0ulqKiQ4uIisrIzKr3fZDKRlZWBQzU9kOrj4sWL/Prrr42wJQXaRUC7Oq7W0Pl3CoHCHEjJAaqfJKjhMptw261X+Qm9uqRl7TYqbqdiorj6pN8cKsZzdbKv+Lumdav7KV+/pp9yt99+O+3bt2+SMto0GZhMJlatWsVLL72Et7c3c+fOpW/fvgQHBzf6vpycdehNWsuBvfrbTlW/6+Luu+9mzpw5ZGVlsXbtWjZu3EhgYCD+/v7s2bOHpKQk3N3d8fT0RFEUy4xjV/8RGY1GAgIC8PHxYe/evSQlJeHq6srw4cN54oknePzxx/H09CQ7Oxt3d3cGDRrE999/z4wZMzAYDBQWFuLq6mrZXsXt11beir8nTJhwzTYqbquqb6U1ffOtqLpv5AAuLi4UFRVV+ue/+nHFb+7VEUYjFOZDfq75Jr6iIkRRIRQXQnERorjI/Lio0NzLqqgQYTIhNFqEiyvC+fKPkwsmJxeEkzPC0dn8W++IUFQoJUVQVIRSVGDeVmEhSnGBeZuFBeblhQWoDGUoQqBCmH8LgcLl32oNKr0jiqMjOOgwCYEJBRNc/qn4uPy5Yr57XaM2r6/RoKi1KFoNikaDotGiaLSoNFpz7zOTAcq7JZuMYDSZn5vKnxvNvdBUKoRKg1CrQK1BqFQI1eUbG1Vq83O1Gp2DjsKCAoTRgMlovPbHZESYyjCZTObPSaWgXL6SVFSXT6qXq/hUivmqUlxuWyovp6hQblGh/EKYUGlUqBU1KkWHWqVc/lGhUlSoVSrLc0WtMXfV1mpBqwMHnfmxgw40WktPOCEEzs7O5ObmWpJMxb9nk8mEyWjEaDQgDAaUy8dSMRlRLnfzVip0+VZMBvMxVqsRl+/3Mak15sdqNabyG0ZV6irORwLFJFCECcVkuua3m4OWpmLTZBAbG0tAQAD+/v4ADBw4kAMHDjRJMtDpdE06hHWPHj0oLCwkICCAoKAg7r77bh588EHGjBlDz5496dy5M1qt1vINvrpv8pMnT+bBBx9k3LhxlvV0Oh2dO3fmL3/5C1OmTKk0L8HixYt59tln+eabbyrNYdAQLi4u+Pn5NWgb9WWroYSFEFBaCg4OjX7PhberK+kXzkNBLuTnmcehKsg1D1aYnwcFuYiCfHNSUlRUbkSgckODopgrkMtKoKgUSkvMcZeWmBv7y3/XxHIHu+bKHe0qtTkxVOyKbCizvpAq1eUbHbXmE275vSyIy4nn8o8oT0qXH1d8rbwRTmB+ranpHM3dpHWOqBUwlpZWvo/HVOFxU1TTqFTmrtpqtfkzK7vcw6+mVULmQbua6/7ry6YNyPv27ePQoUM89thjgLlb6JkzZ5g1a1aN67XG+QyaipzPoOVr7jIJk+lyT7AS8wmt/M70y0mgTvXfFbshXz5ZeXl6kpmbd/mk7wBaTZN1KTZ3GjBd6b1R3oPDVL68wu+rl5WVme/cv3wVWPHKkKKiK4+Li9A5OlJiMFQY9kV7+XhdNSSMRgs6HTjoUXR681WG7vJVh4MeypdptebEXFS+P/NVqCi/Gi0uvBKD0WC+ctFqQXv5plONg+W5Ur5cq4WQTihWdBCx2wbk6OhooqOjAVi8eLFlJrFyKSkpaDTWFcXa97UmjV0mnU53zTFuLhqNxmb7bir2ViaNRoOvn/xS1ZLV52/OpmdGLy8vMjKuNIhmZGTg5eV1zfuioqKIioqyPL8645WUlKC2YsC2lvSBnzhxgqeeeqrSMp1Ox8aNG+u0naYoU0lJic2+ncsrg5bP3soD9lemVndl0KlTJ5KSkkhNTcXLy4s9e/Zcc4K0Riu8VYLu3buzefNmW4dRpdZ4PCVJahibJgO1Ws3MmTN57bXXMJlMjBgxgpCQkDpvR6VSYTAY7LIKqLkZDAZUcrwhSWpzbH727N27N717927QNvR6PcXFxZSUlNTYOKbT6Sgpsa85hxuzTEIIVCoVer2+UbYnSVLrYfNk0BgURcHR0bHW99lbvSDYZ5kkSWp+sj5AkiRJkslAkiRJkslAkiRJopUOYS1JkiQ1rjZ1ZTBnzhxbh9Do7K1M9lYesL8y2Vt5wP7KVJ/ytKlkIEmSJFVNJgNJkiSpbSWDiuMb2Qt7K5O9lQfsr0z2Vh6wvzLVpzyyAVmSJElqW1cGkiRJUtVkMpAkSZLsY2wiaxw6dIiPP/4Yk8nEqFGjmDhxoq1DapDZs2ej1+tRqVSo1WoWL15s65Dq7N133yUmJgZ3d3eWLl0KQH5+PsuWLSMtLQ1fX1+efvppXFxcbBypdaoqz1dffcWWLVtwczPPTnXfffc1eGDG5pSens6KFSvIzs5GURSioqK49dZbW+3nVF15WvPnVFpayrx58zAYDBiNRvr378/kyZNJTU1l+fLl5OXlERYWxpNPPlnzyM6iDTAajeL//u//RHJysigrKxN/+9vfREJCgq3DapAnnnhC5OTk2DqMBjl27Jg4e/aseOaZZyzLVq9eLb799lshhBDffvutWL16tY2iq7uqyrN27Vrx3Xff2TCqhsnMzBRnz54VQghRWFgonnrqKZGQkNBqP6fqytOaPyeTySSKioqEEEKUlZWJuXPnilOnTomlS5eK3bt3CyGEWLlypfj5559r3E6bqCaKjY0lICAAf39/NBoNAwcO5MCBA7YOq80LDw+/5tvkgQMHGDZsGADDhg1rVZ9TVeVp7Tw9PQkLCwPA0dGRoKAgMjMzW+3nVF15WjNFUSzDzhuNRoxGI4qicOzYMfr37w/A8OHDa/2M2kQ1UWZmJt7e3pbn3t7enDlzxoYRNY7XXnsNgJtvvtluusbl5OTg6ekJgIeHBzk5OTaOqOF+/vlndu7cSVhYGNOmTWu1CSM1NZW4uDg6d+5sF59TxfKcPHmyVX9OJpOJF154geTkZG655Rb8/f1xcnKyTAfs5eVVa9JrE8nAHr366qt4eXmRk5PDggULCAwMJDw83NZhNSpFUWqcrKg1GD16NJMmTQJg7dq1fPbZZzzxxBM2jqruiouLWbp0KdOnT8fJyanSa63xc7q6PK39c1KpVLzxxhsUFBTw5ptvkpiYWPdtNEFcLY6XlxcZGRmW5xkZGXh5edkwooYrj9/d3Z1+/foRGxtr44gah7u7O1lZWQBkZWVZGvRaKw8PD1QqFSqVilGjRnH27Flbh1RnBoOBpUuXMmTIECIjI4HW/TlVVR57+JwAnJ2d6dGjB6dPn6awsBCj0QiYa0dqO+e1iWTQqVMnkpKSSE1NxWAwsGfPHvr27WvrsOqtuLiYoqIiy+MjR44QGhpq46gaR9++fdmxYwcAO3bsoF+/fjaOqGHKT5gA+/fvr9cc37YkhOC9994jKCiI8ePHW5a31s+puvK05s8pNzeXgoICwNyz6MiRIwQFBdGjRw/27dsHwPbt22s957WZO5BjYmL49NNPMZlMjBgxgjvvvNPWIdVbSkoKb775JmBuMBo8eHCrLM/y5cs5fvw4eXl5uLu7M3nyZPr168eyZctIT09vVV0WoeryHDt2jPPnz6MoCr6+vjzyyCOWuvbW4OTJk7zyyiuEhoZaqoLuu+8+unTp0io/p+rK8+uvv7bazyk+Pp4VK1ZgMpkQQjBgwAAmTZpESkoKy5cvJz8/n44dO/Lkk0+i1Wqr3U6bSQaSJElS9dpENZEkSZJUM5kMJEmSJJkMJEmSJJkMJEmSJGQykCRJkpDJQJKa1OTJk0lOTrZ1GJJUKzkchdRmzJ49m+zsbFSqK9+Bhg8fzqxZs2wYVdV+/vlnMjIyuP/++5k3bx4zZ86kffv2tg5LsmMyGUhtygsvvEDPnj1tHUatzp07R+/evTGZTFy6dIng4GBbhyTZOZkMJAnz7fpbtmyhQ4cO7Ny5E09PT2bNmsX1118PmMd2+eCDDzh58iQuLi7cfvvtlpFiTSYT69evZ9u2beTk5NCuXTuee+45fHx8ADhy5AgLFy4kNzeXwYMHM2vWrFoHdjt37hyTJk0iMTERX19fy+iTktRUZDKQpMvOnDlDZGQkq1atYv/+/bz55pusWLECFxcX3nrrLUJCQli5ciWJiYm8+uqrBAQEEBERwcaNG/n111+ZO3cu7dq1Iz4+Hp1OZ9luTEwMixYtoqioiBdeeIG+ffvSq1eva/ZfVlbGww8/jBCC4uJinnvuOQwGAyaTienTpzNhwoRWOeyI1DrIZCC1KW+88Ualb9lTpkyxfMN3d3dn3LhxKIrCwIED2bBhAzExMYSHh3Py5EnmzJmDg4MDHTp0YNSoUezYsYOIiAi2bNnClClTCAwMBKBDhw6V9jlx4kScnZ0tI0qeP3++ymSg1Wr55JNP2LJlCwkJCUyfPp0FCxZw77330rlz5yY7JpIEMhlIbcxzzz1XbZuBl5dXpeobX19fMjMzycrKwsXFBUdHR8trPj4+lmGOMzIy8Pf3r3afHh4elsc6nY7i4uIq37d8+XIOHTpESUkJWq2Wbdu2UVxcTGxsLO3atWPRokV1Kaok1YlMBpJ0WWZmJkIIS0JIT0+nb9++eHp6kp+fT1FRkSUhpKenW8aH9/b2JiUlpcHDiP/1r3/FZDLxyCOP8P777/PHH3+wd+9ennrqqYYVTJKsIO8zkKTLcnJy2LRpEwaDgb1793Lp0iVuvPFGfHx86Nq1K59//jmlpaXEx8ezbds2hgwZAsCoUaNYu3YtSUlJCCGIj48nLy+vXjFcunQJf39/VCoVcXFxdOrUqTGLKEnVklcGUpvy+uuvV7rPoGfPnjz33HMAdOnShaSkJGbNmoWHhwfPPPMMrq6uAPzlL3/hgw8+4NFHH8XFxYW7777bUt00fvx4ysrKWLBgAXl5eQQFBfG3v/2tXvGdO3eOjh07Wh7ffvvtDSmuJFlNzmcgSVzpWvrqq6/aOhRJsglZTSRJkiTJZCBJkiTJaiJJkiQJeWUgSZIkIZOBJEmShEwGkiRJEjIZSJIkSchkIEmSJAH/DwUeI+Tos5jvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_training(H):\n",
    "# construct a plot that plots and saves the training history\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    plt.plot(H.history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(H.history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.plot(H.history.history[\"acc\"], label=\"train_acc\")\n",
    "    plt.plot(H.history.history[\"val_acc\"], label=\"val_acc\")\n",
    "    plt.title(\"Training Loss and Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "plot_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "32\n",
      "64\n",
      "96\n",
      "128\n",
      "160\n",
      "192\n",
      "224\n",
      "256\n",
      "288\n",
      "320\n",
      "352\n",
      "384\n",
      "416\n",
      "448\n",
      "480\n",
      "512\n",
      "544\n",
      "576\n",
      "608\n",
      "640\n",
      "672\n",
      "704\n",
      "736\n",
      "768\n",
      "800\n",
      "832\n",
      "864\n",
      "896\n",
      "928\n",
      "960\n",
      "992\n",
      "1024\n",
      "1056\n",
      "1088\n",
      "1120\n",
      "1152\n",
      "1184\n",
      "1216\n",
      "1248\n",
      "1280\n",
      "1312\n",
      "1344\n",
      "1376\n",
      "1408\n",
      "1440\n",
      "1472\n",
      "1504\n",
      "1536\n",
      "1568\n",
      "1600\n",
      "1632\n",
      "1664\n",
      "1696\n",
      "1728\n",
      "1760\n",
      "1792\n",
      "1824\n",
      "1856\n",
      "1888\n",
      "1920\n",
      "1952\n",
      "1984\n",
      "2016\n",
      "2048\n",
      "2080\n",
      "2112\n",
      "2144\n",
      "2176\n",
      "2208\n",
      "2240\n",
      "2272\n",
      "2304\n",
      "2336\n",
      "2368\n",
      "2400\n",
      "2432\n",
      "2464\n",
      "2496\n",
      "2528\n",
      "2560\n",
      "2592\n",
      "2624\n",
      "2656\n",
      "2688\n",
      "2720\n",
      "2752\n",
      "2784\n",
      "2816\n",
      "2848\n",
      "2880\n",
      "2912\n",
      "2944\n",
      "2976\n"
     ]
    }
   ],
   "source": [
    "# Modify paths as per your need\n",
    "test_path = \"<REPLACE-PATH>\\\\Kinship Recognition Starter dh2868_al4008\\\\test\\\\\"\n",
    "\n",
    "# model = model3()\n",
    "# model.load_weights(\"C:\\\\Users\\\\DavidHeagy\\\\Desktop\\\\Kinship Recognition Starter dh2868_al4008-20210809T164504Z-001\\\\Kinship Recognition Starter dh2868_al4008\\\\new_face_2021-08-15 03:31:50.410707.h5\")\n",
    "\n",
    "submission = pd.read_csv(r'<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\test_ds.csv')\n",
    "predictions = []\n",
    "\n",
    "for i in range(0, len(submission.p1.values), 32):\n",
    "    print(i)\n",
    "    X1 = submission.p1.values[i:i+32]\n",
    "    X1_vgg = np.array([read_img_vgg(test_path + x) for x in X1])\n",
    "    X1_fn = np.array([read_img_fn(test_path + x) for x in X1])\n",
    "    X2 = submission.p2.values[i:i+32]\n",
    "    X2_vgg = np.array([read_img_vgg(test_path + x) for x in X2])\n",
    "    X2_fn = np.array([read_img_fn(test_path + x) for x in X2])\n",
    "    pred = model.predict([X1_vgg, X2_vgg, X1_fn, X2_fn]).ravel().tolist()\n",
    "    predictions += pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1697 1303\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "n  = 0\n",
    "for i in predictions:\n",
    "    if i>=.5:\n",
    "        p+=1\n",
    "    else:\n",
    "        n+=1\n",
    "print(p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5656666666666667\n",
      "0.5653333333333334\n",
      "0.5653333333333334\n",
      "0.5653333333333334\n",
      "0.565\n",
      "0.5646666666666667\n",
      "0.5643333333333334\n",
      "0.5643333333333334\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.564\n",
      "0.5636666666666666\n",
      "0.5636666666666666\n",
      "0.5633333333333334\n",
      "0.5633333333333334\n",
      "0.5626666666666666\n",
      "0.5626666666666666\n",
      "0.5623333333333334\n",
      "0.5623333333333334\n",
      "0.5623333333333334\n",
      "0.5623333333333334\n",
      "0.5623333333333334\n",
      "0.562\n",
      "0.562\n",
      "0.562\n",
      "0.562\n",
      "0.562\n",
      "0.562\n",
      "0.562\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5616666666666666\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.5613333333333334\n",
      "0.561\n",
      "0.5606666666666666\n",
      "0.5606666666666666\n",
      "0.5606666666666666\n",
      "0.5606666666666666\n",
      "0.5603333333333333\n",
      "0.5603333333333333\n",
      "0.5603333333333333\n",
      "0.56\n",
      "0.56\n",
      "0.56\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5596666666666666\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.5593333333333333\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.559\n",
      "0.5586666666666666\n",
      "0.5586666666666666\n",
      "0.5586666666666666\n",
      "0.5586666666666666\n",
      "0.5586666666666666\n",
      "0.5586666666666666\n",
      "0.5583333333333333\n",
      "0.5583333333333333\n",
      "0.558\n",
      "0.5576666666666666\n",
      "0.5576666666666666\n",
      "0.5573333333333333\n",
      "0.5573333333333333\n",
      "0.5573333333333333\n",
      "0.557\n",
      "0.5566666666666666\n",
      "0.5566666666666666\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5563333333333333\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5556666666666666\n",
      "0.5553333333333333\n",
      "0.5553333333333333\n",
      "0.5553333333333333\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.555\n",
      "0.5546666666666666\n",
      "0.5543333333333333\n",
      "0.5543333333333333\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.554\n",
      "0.5533333333333333\n",
      "0.553\n",
      "0.553\n",
      "0.5526666666666666\n",
      "0.5526666666666666\n",
      "0.5526666666666666\n",
      "0.5526666666666666\n",
      "0.5526666666666666\n",
      "0.5523333333333333\n",
      "0.552\n",
      "0.552\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5516666666666666\n",
      "0.5513333333333333\n",
      "0.5513333333333333\n",
      "0.5513333333333333\n",
      "0.5513333333333333\n",
      "0.5513333333333333\n",
      "0.551\n",
      "0.551\n",
      "0.551\n",
      "0.5506666666666666\n",
      "0.5506666666666666\n",
      "0.5503333333333333\n",
      "0.55\n",
      "0.55\n",
      "0.55\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5496666666666666\n",
      "0.5493333333333333\n",
      "0.5493333333333333\n",
      "0.5493333333333333\n",
      "0.549\n",
      "0.549\n",
      "0.549\n",
      "0.549\n",
      "0.5486666666666666\n",
      "0.5483333333333333\n",
      "0.5483333333333333\n",
      "0.5483333333333333\n",
      "0.5483333333333333\n",
      "0.5483333333333333\n",
      "0.5483333333333333\n",
      "0.548\n",
      "0.548\n",
      "0.548\n",
      "0.548\n",
      "0.548\n",
      "0.548\n",
      "0.5476666666666666\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5473333333333333\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5466666666666666\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.5463333333333333\n",
      "0.546\n",
      "0.546\n",
      "0.546\n",
      "0.546\n",
      "0.546\n",
      "0.546\n",
      "0.546\n",
      "0.5453333333333333\n",
      "0.5453333333333333\n",
      "0.5453333333333333\n",
      "0.5453333333333333\n",
      "0.5453333333333333\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.545\n",
      "0.5446666666666666\n",
      "0.5443333333333333\n",
      "0.544\n",
      "0.544\n",
      "0.5433333333333333\n",
      "0.543\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5426666666666666\n",
      "0.5423333333333333\n",
      "0.5423333333333333\n",
      "0.5423333333333333\n",
      "0.5423333333333333\n",
      "0.5423333333333333\n",
      "0.5423333333333333\n",
      "0.542\n",
      "0.5416666666666666\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5413333333333333\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5406666666666666\n",
      "0.5403333333333333\n",
      "0.5403333333333333\n",
      "0.5403333333333333\n",
      "0.5403333333333333\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.54\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5396666666666666\n",
      "0.5393333333333333\n",
      "0.5393333333333333\n",
      "0.539\n",
      "0.539\n",
      "0.539\n",
      "0.539\n",
      "0.539\n",
      "0.539\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5386666666666666\n",
      "0.5383333333333333\n",
      "0.5383333333333333\n",
      "0.5383333333333333\n",
      "0.5383333333333333\n",
      "0.5383333333333333\n",
      "0.538\n",
      "0.538\n",
      "0.538\n",
      "0.5376666666666666\n",
      "0.5376666666666666\n",
      "0.5373333333333333\n",
      "0.5373333333333333\n",
      "0.5373333333333333\n",
      "0.5373333333333333\n",
      "0.537\n",
      "0.537\n",
      "0.537\n",
      "0.537\n",
      "0.537\n",
      "0.5366666666666666\n",
      "0.5366666666666666\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.5363333333333333\n",
      "0.536\n",
      "0.536\n",
      "0.5356666666666666\n",
      "0.5356666666666666\n",
      "0.5356666666666666\n",
      "0.5356666666666666\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.5353333333333333\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.535\n",
      "0.5346666666666666\n",
      "0.5343333333333333\n",
      "0.534\n",
      "0.534\n",
      "0.534\n",
      "0.534\n",
      "0.534\n",
      "0.534\n",
      "0.534\n",
      "0.5336666666666666\n",
      "0.5336666666666666\n",
      "0.5336666666666666\n",
      "0.5336666666666666\n",
      "0.5333333333333333\n",
      "0.5333333333333333\n",
      "0.5333333333333333\n",
      "0.5333333333333333\n",
      "0.533\n",
      "0.533\n",
      "0.5326666666666666\n",
      "0.5326666666666666\n",
      "0.5323333333333333\n",
      "0.5323333333333333\n",
      "0.532\n",
      "0.532\n",
      "0.5316666666666666\n",
      "0.5316666666666666\n",
      "0.5316666666666666\n",
      "0.5313333333333333\n",
      "0.5313333333333333\n",
      "0.5313333333333333\n",
      "0.5313333333333333\n",
      "0.531\n",
      "0.531\n",
      "0.531\n",
      "0.531\n",
      "0.531\n",
      "0.531\n",
      "0.5306666666666666\n",
      "0.5306666666666666\n",
      "0.5306666666666666\n",
      "0.5303333333333333\n",
      "0.53\n",
      "0.53\n",
      "0.5296666666666666\n",
      "0.5296666666666666\n",
      "0.5293333333333333\n",
      "0.5293333333333333\n",
      "0.5293333333333333\n",
      "0.5293333333333333\n",
      "0.5293333333333333\n",
      "0.529\n",
      "0.529\n",
      "0.5286666666666666\n",
      "0.5286666666666666\n",
      "0.5283333333333333\n",
      "0.5283333333333333\n",
      "0.5283333333333333\n",
      "0.5283333333333333\n",
      "0.5283333333333333\n",
      "0.528\n",
      "0.528\n",
      "0.528\n",
      "0.5276666666666666\n",
      "0.5276666666666666\n",
      "0.5276666666666666\n",
      "0.5273333333333333\n",
      "0.5273333333333333\n",
      "0.5273333333333333\n",
      "0.5273333333333333\n",
      "0.527\n",
      "0.527\n",
      "0.527\n",
      "0.527\n",
      "0.527\n",
      "0.527\n",
      "0.527\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5266666666666666\n",
      "0.5263333333333333\n",
      "0.5263333333333333\n",
      "0.5263333333333333\n",
      "0.5263333333333333\n",
      "0.5263333333333333\n",
      "0.5263333333333333\n",
      "0.526\n",
      "0.526\n",
      "0.5256666666666666\n",
      "0.5256666666666666\n",
      "0.5256666666666666\n",
      "0.5253333333333333\n",
      "0.5253333333333333\n",
      "0.5253333333333333\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5246666666666666\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.5243333333333333\n",
      "0.524\n",
      "0.524\n",
      "0.524\n",
      "0.524\n",
      "0.5233333333333333\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5226666666666666\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.5223333333333333\n",
      "0.522\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.5213333333333333\n",
      "0.521\n",
      "0.521\n",
      "0.521\n",
      "0.521\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5206666666666667\n",
      "0.5203333333333333\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.52\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.5193333333333333\n",
      "0.519\n",
      "0.519\n",
      "0.519\n",
      "0.519\n",
      "0.519\n",
      "0.519\n",
      "0.5186666666666667\n",
      "0.5183333333333333\n",
      "0.5183333333333333\n",
      "0.5183333333333333\n",
      "0.5183333333333333\n",
      "0.518\n",
      "0.518\n",
      "0.518\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5176666666666667\n",
      "0.5173333333333333\n",
      "0.5173333333333333\n",
      "0.517\n",
      "0.517\n",
      "0.5166666666666667\n",
      "0.5166666666666667\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.5163333333333333\n",
      "0.516\n",
      "0.516\n",
      "0.516\n",
      "0.516\n",
      "0.516\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5156666666666667\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.5153333333333333\n",
      "0.515\n",
      "0.515\n",
      "0.515\n",
      "0.515\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5146666666666667\n",
      "0.5143333333333333\n",
      "0.5143333333333333\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.514\n",
      "0.5136666666666667\n",
      "0.5136666666666667\n",
      "0.513\n",
      "0.5126666666666667\n",
      "0.512\n",
      "0.512\n",
      "0.512\n",
      "0.5116666666666667\n",
      "0.5113333333333333\n",
      "0.5113333333333333\n",
      "0.511\n",
      "0.511\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5106666666666667\n",
      "0.5103333333333333\n",
      "0.5103333333333333\n",
      "0.5103333333333333\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.51\n",
      "0.5096666666666667\n",
      "0.5096666666666667\n",
      "0.5096666666666667\n",
      "0.5096666666666667\n",
      "0.509\n",
      "0.509\n",
      "0.509\n",
      "0.509\n",
      "0.509\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5086666666666667\n",
      "0.5083333333333333\n",
      "0.5083333333333333\n",
      "0.5083333333333333\n",
      "0.5083333333333333\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.508\n",
      "0.5076666666666667\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.5073333333333333\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.507\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.5066666666666667\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.506\n",
      "0.5056666666666667\n",
      "0.5056666666666667\n",
      "0.5056666666666667\n",
      "0.5053333333333333\n",
      "0.5053333333333333\n",
      "0.5053333333333333\n",
      "0.505\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5046666666666667\n",
      "0.5043333333333333\n",
      "0.504\n",
      "0.504\n",
      "0.504\n",
      "0.504\n",
      "0.504\n",
      "0.5036666666666667\n",
      "0.5036666666666667\n",
      "0.5036666666666667\n",
      "0.5036666666666667\n",
      "0.5036666666666667\n",
      "0.5036666666666667\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5033333333333333\n",
      "0.5026666666666667\n",
      "0.5023333333333333\n",
      "0.5023333333333333\n",
      "0.5023333333333333\n",
      "0.5023333333333333\n",
      "0.502\n",
      "0.502\n",
      "0.502\n",
      "0.502\n",
      "0.502\n",
      "0.502\n",
      "0.5016666666666667\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.5013333333333333\n",
      "0.501\n"
     ]
    }
   ],
   "source": [
    "# postprocessing used \n",
    "epsilon = .0001\n",
    "delta = .001\n",
    "n = len(predictions)\n",
    "while True:\n",
    "    pos_count = np.sum(np.rint(predictions))\n",
    "    prop_pos = pos_count / n\n",
    "    print(prop_pos)\n",
    "    \n",
    "    if prop_pos <= 0.5 + delta and prop_pos >= 0.5 -delta:\n",
    "        break\n",
    "        \n",
    "    for i in range(n):\n",
    "        if prop_pos < 0.5: # 0 most popular class\n",
    "            if predictions[i] < 0.5:\n",
    "                predictions[i] += epsilon\n",
    "        else:\n",
    "            if predictions[i] >= 0.5:\n",
    "                predictions[i] -=epsilon\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i,v in enumerate(predictions):\n",
    "    if v>=.5:\n",
    "        predictions[i]=1\n",
    "    else:\n",
    "        predictions[i]=0\n",
    "# print(p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503 1497\n"
     ]
    }
   ],
   "source": [
    "p = 0\n",
    "n  = 0\n",
    "for i in predictions:\n",
    "    if i>=.5:\n",
    "        p+=1\n",
    "    else:\n",
    "        n+=1\n",
    "print(p,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
    "submissionfile = pd.DataFrame(data=d)\n",
    "submissionfile = submissionfile.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissionfile.astype(\"int64\").to_csv(r\"<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\dh2868_al4008_{}.csv\".format(4), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVERYTHING BELOW IS IRRELEVANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5k69UJzvb26"
   },
   "source": [
    "Define a data generator. Here our data generator will generate a batch of examples which will be used by our model in training. It will generate two images, one for each in the pair as well as a label associated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1628469239354,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "kcAZG7JYAdhY"
   },
   "outputs": [],
   "source": [
    "def gen(list_tuples, person_to_images_map, batch_size=16):\n",
    "    ppl = list(person_to_images_map.keys())\n",
    "    while True:\n",
    "        batch_tuples = sample(list_tuples, batch_size)\n",
    "        \n",
    "        # All the samples are taken from train_ds.csv, labels are in the labels column\n",
    "        labels = []\n",
    "        for tup in batch_tuples:\n",
    "            labels.append(tup[2])\n",
    "\n",
    "        X1 = [x[0] for x in batch_tuples]\n",
    "        X1 = np.array([read_img(train_folders_path + x) for x in X1])\n",
    "\n",
    "        X2 = [x[1] for x in batch_tuples]\n",
    "        X2 = np.array([read_img(train_folders_path + x) for x in X2])\n",
    "\n",
    "        yield [X1, X2], np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvPvyRzBw-nt"
   },
   "source": [
    "Here is an ensemble model built with two resnet-50 architectures, pre-trained, with which we can apply transfer leraning on. This model achieves the baseline and the goal is to expand on this work. There have been papers exploring different architectures as well as introducing BatchNormalization among many other techniques to improve how well the model recognizes kinship between two faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1628469239355,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "3BBZJpieAi7Y"
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = VGGFace(model='resnet50', include_top=False)\n",
    "\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x2 = base_model(input_2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x = Multiply()([x1, x2])\n",
    "\n",
    "    x = Concatenate(axis=-1)([x, x3])\n",
    "\n",
    "    x = Dense(100, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.00001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 125,
     "status": "ok",
     "timestamp": 1628469794668,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "96d6p2T0yqrQ"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Flatten\n",
    "from tensorflow.keras.applications import resnet\n",
    "def new_model():\n",
    "    input_1 = Input(shape=(224, 224, 3))\n",
    "    input_2 = Input(shape=(224, 224, 3))\n",
    "\n",
    "    base_model = VGGFace( include_top=False)\n",
    "\n",
    "    for x in base_model.layers[:-3]:\n",
    "        x.trainable = True\n",
    "\n",
    "    x1 = base_model(input_1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    x2 = base_model(input_2)\n",
    "    x2 = BatchNormalization()(x2)\n",
    "\n",
    "    x1 = Concatenate(axis=-1)([GlobalMaxPool2D()(x1), GlobalAvgPool2D()(x1)])\n",
    "    x2 = Concatenate(axis=-1)([GlobalMaxPool2D()(x2), GlobalAvgPool2D()(x2)])\n",
    "\n",
    "    x3 = Subtract()([x1, x2])\n",
    "    x3 = Multiply()([x3, x3])\n",
    "\n",
    "    x = Multiply()([x1, x2])\n",
    "\n",
    "    x = Concatenate(axis=-1)([x, x3])\n",
    "    x = Flatten(name='flatten')(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    out = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model([input_1, input_2], out)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=Adam(0.0001))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DC3TNCLWx5RC"
   },
   "source": [
    "Save the best model to your drive after each training epoch so that you can come back to it. ReduceLROnPlateau reduces the learning rate when a metric has stopped improving, in this case the validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1352,
     "status": "ok",
     "timestamp": 1628469798989,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "3YEQ0Q6Ui6NP",
    "outputId": "6e800de4-4ac2-43b7-b60e-1f2109e47dd6"
   },
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "date_t = datetime.datetime.now()\n",
    "\n",
    "# file_path = \"/content/drive/MyDrive/Kinship Recognition Starter dh2868_al4008/vgg_face.h5\"\n",
    "new_file_path = r\"<REPLACE-PATH>\\Kinship Recognition Starter dh2868_al4008\\new_face_{}.h5\".format(date_t)\n",
    "checkpoint = ModelCheckpoint(new_file_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='max')\n",
    "\n",
    "reduce_on_plateau = ReduceLROnPlateau(monitor=\"val_acc\", mode=\"max\", factor=0.1, patience=20, verbose=1)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_on_plateau]\n",
    "\n",
    "model = baseline_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 843976,
     "status": "ok",
     "timestamp": 1628467817364,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "zIl075HvEfAf",
    "outputId": "61e82e4b-130f-4279-d8c3-dce69e52ae4b"
   },
   "outputs": [],
   "source": [
    "# Modify paths as per your need\n",
    "test_path = \"/content/drive/MyDrive/Kinship Recognition Starter dh2868_al4008/test/\"\n",
    "\n",
    "# model = baseline_model()\n",
    "# model.load_weights(\"/gdrive/MyDrive/vgg_face.h5\")\n",
    "\n",
    "submission = pd.read_csv('/content/drive/MyDrive/Kinship Recognition Starter dh2868_al4008/test_ds.csv')\n",
    "predictions = []\n",
    "\n",
    "for i in range(0, len(submission.p1.values), 32):\n",
    "    print(i)\n",
    "    X1 = submission.p1.values[i:i+32]\n",
    "    X1 = np.array([read_img(test_path + x) for x in X1])\n",
    "\n",
    "    X2 = submission.p2.values[i:i+32]\n",
    "    X2 = np.array([read_img(test_path + x) for x in X2])\n",
    "\n",
    "    pred = model.predict([X1, X2]).ravel().tolist()\n",
    "    predictions += pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXyzufq7iZ1g"
   },
   "source": [
    "The final predictions will need to be rounded: EG 0.01 rounded to 0 and 0.78 rounded to 1. The simple .round() function is sufficient as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1628467818128,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "JkFEH-uva9c_"
   },
   "outputs": [],
   "source": [
    "d = {'index': np.arange(0, 3000, 1), 'label':predictions}\n",
    "submissionfile = pd.DataFrame(data=d)\n",
    "submissionfile = submissionfile.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1628467818128,
     "user": {
      "displayName": "David Heagy",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjIBAJzlCPtnjza79sFLmAHSwmxjL0srIsgbP4Y1YzPB08JMvdaZnP5AuQUOXckZqrUpkQJI0rblqajd9XV-ZYZGbz5wADD_AR7OUSrPG7En7cvqgI239mHh7aRimeWp4xd2Pi_Bu9OgUjlWi4NoXma_2fHJz8iu-hi5lio1QLdp7J9igf2BIXOHW8jW3SiA-0bFp5mAEoCTnoIZflUpmdSewSE9eXPzIYUFZlsxF09uFxmJQuj2JEIY7p6dKIf4JAqmlfn3uQIN2YNPzmvF4d1xyrPl-EKG3OkGqC_ovYcEyXt0YkbCtrIxaUTCWbyjx10ng60sj7PR_6fdomS_iw2E-wpM7QMmppt2cl52SUsAfR0X7LVzqtTm_Hsd3DVPjBZ4nn284vk41AY_W-ysfe6R_ZOyo55O9mtm2VmWVY9AO1-nFb0PdfhRM_lbAsAJmetOUItRpOrp9RFnKjJxWMO-7N8pMF6noiSQbRtPB036K1YajLyJvzrnK74N1AyhcStuZqapkhp2Xj5ZljF5_UN0ZLnibdYGRrIbgu6kBPOkCldTMAbA5BGi3zaAXwOln-ySZqjLin1Yotn1BvuM6wgjuTLmCHmG29FpjGYOzoVVL-clwOLo5N3bLhRCi8-RaWlRscPc-WFl0y29DiuULmNLKUIr9bNqVIZVS71XMMj0oNJuYP2Jcvt3GY7rRHLRaj5svyATPk1kw1odMhZXl0c-g00cUlki9dvzSEkCcomipj0bevVPCyg6qIUQOdvEMRiMA=s64",
      "userId": "04055744733312252090"
     },
     "user_tz": 240
    },
    "id": "4EbbawPYrchP"
   },
   "outputs": [],
   "source": [
    "submissionfile.astype(\"int64\").to_csv(\"/content/drive/MyDrive/Kinship Recognition Starter dh2868_al4008/dh2868_al4008_{}.csv\".format(date_t), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNodVS9W4NMB"
   },
   "source": [
    "At this point, download the CSV and submit it on Kaggle to score your predictions.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "my dh2868_al4008.ipynb",
   "provenance": [
    {
     "file_id": "15ISg13nHCoJN5rGJRWUJZSYZq69s8rQb",
     "timestamp": 1628203675499
    },
    {
     "file_id": "1G7R1NDwBj7Fa6He3263Ep9F5FgehVV4q",
     "timestamp": 1628198296266
    }
   ],
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
